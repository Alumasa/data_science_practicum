{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('/datasets/travel_insurance_us.csv')\n",
    "\n",
    "# < write code here >\n",
    "train, valid = train_test_split(data, test_size=0.25, random_state=12345)\n",
    "features_train = train.drop('Claim', axis=1)\n",
    "target_train = train['Claim']\n",
    "features_valid = valid.drop('Claim', axis=1)\n",
    "target_valid = valid['Claim']\n",
    "print(features_train.shape)\n",
    "print(features_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy variables\n",
    "print(pd.get_dummies(data['Gender']).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe = pd.get_dummies(data, drop_first=True)\n",
    "print(data_ohe.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data_ohe['Claim']\n",
    "features = data_ohe.drop('Claim', axis=1)\n",
    "\n",
    "# < write code here >\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, target, test_size=0.25, random_state=12345)\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model.fit(features_train, target_train)\n",
    "print(\"Trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of this class\n",
    "encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a31c018dd841>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#fit the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#fit the data\n",
    "encoder.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the data\n",
    "data_ordinal = encoder.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column names\n",
    "data_ordinal = pd.DataFrame(encoder.transform(data), columns=data.columns)\n",
    "\n",
    "#combine fit and transform\n",
    "data_ordinal = pd.DataFrame(encoder.fit_transform(data), columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "print(\"Trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance of the class and tune it using the training data. \n",
    "#The tuning process implies calculating the mean and variance:\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the training and validation set\n",
    "features_train_scaled = scaler.transform(features_train)\n",
    "features_valid_scaled = scaler.transform(features_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['Duration', 'Net Sales', 'Commission (in value)', 'Age']\n",
    "\n",
    "# < write code here >\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "#print(features_train[['Duration', 'Net Sales', 'Commission (in value)', 'Age']].head())\n",
    "#print(features_train[numeric].head())\n",
    "print(features_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine categorical feature encoding and numerical feature scaling\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv('travel_insurance_us.csv')\n",
    "\n",
    "data_ohe = pd.get_dummies(data, drop_first=True)\n",
    "target = data_ohe['Claim']\n",
    "features = data_ohe.drop('Claim', axis=1)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345)\n",
    "\n",
    "numeric = ['Duration', 'Net Sales', 'Commission (in value)', 'Age']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification  Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy of DecisionTree Classifier\n",
    "model = DecisionTreeClassifier(random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "accuracy_valid = accuracy_score(predicted_valid, target_valid)\n",
    "print(accuracy_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_frequency = data['Claim'].value_counts(normalize=True)# < write code here >)\n",
    "print(class_frequency)\n",
    "class_frequency.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make value_counts() work,\n",
    "# we converted results to pd.Series \n",
    "predicted_valid = pd.Series(model.predict(features_valid))\n",
    "\n",
    "# < write code here >\n",
    "class_frequency = predicted_valid.value_counts(normalize=True)\n",
    "print(class_frequency)\n",
    "class_frequency.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a constant model: it predicts class \"0\" for any observation:\n",
    "target_pred_constant = pd.Series(0, index=target.index)\n",
    "\n",
    "print(accuracy_score(target, target_pred_constant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "target = pd.Series([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1])\n",
    "predictions = pd.Series([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1])\n",
    "((target == 1) & (predictions == 1).values).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Negative Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(((target == 0) & (predictions == 0)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(((target == 0) & (predictions == 1)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Negative Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(((target == 1) & (predictions == 0)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "- When TP, FP, TN, FN are collected into a table, it is called a **confusion matrix**.\n",
    "- The confusion matrix allows you to visualize the results of calculating the precision and recall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 3]\n",
      " [2 5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(target, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "- Recall reveals the portion of positive answers identified by the model\n",
    "- Recall is the share of TP among all answers that have a true label of 1. \n",
    "    - We want the recall value to be close to 1. \n",
    "        - This would mean that the model is good at identifying true positives. \n",
    "            - If it is closer to zero, the model needs to be checked and fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score# < write code here >\n",
    "print(recall_score(target_valid, predicted_valid))# 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(target, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "- Another metric for evaluating the quality of a target class prediction is precision.\n",
    "    - Precision measures how many negative answers the model found while searching for positive ones. \n",
    "        - The more negative answers are found, the lower the precision.\n",
    "        - We need the precision to be close to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score# < write code here >\n",
    "print(precision_score(target_valid. predicted_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(target,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "- Aggregating metrics — one of which is the F1 score — help to control recall and precision simultaneously. \n",
    "    - This is the harmonic mean of recall and precision. \n",
    "    - In F1, 1 means that the ratio of recall to precision is 1:1.\n",
    "\n",
    "- It's important to understand that when recall or precision is close to zero, the harmonic mean itself approaches 0.\n",
    "\n",
    "*If a positive class is poorly predicted on one of the scales (Recall or Precision), then an F1 score close to zero will show that the prediction of class 1 has failed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7142857142857143\n",
      "Precision: 0.625\n",
      "F1 score, F1 is mean: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(target, predictions)# < write code here >\n",
    "recall = recall_score(target, predictions)# < write code here >\n",
    "f1 = (2 * precision * recall) / (precision + recall)# < write code here >\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"F1 score, F1 is mean:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(target_valid, predicted_valid)) # 0.069"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weights Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=12345, solver='liblinear', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data = pd.read_csv('/datasets/travel_insurance_us_preprocessed.csv')\n",
    "\n",
    "target = data['Claim']\n",
    "features = data.drop('Claim', axis=1)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345)\n",
    "\n",
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    \n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345)\n",
    "    \n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "features_upsampled, target_upsampled = upsample(features_train, target_train, 10)\n",
    "\n",
    "# < write code here >\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model.fit(features_upsampled, target_upsampled)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "print(\"F1:\", f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data = pd.read_csv('/datasets/travel_insurance_us_preprocessed.csv')\n",
    "\n",
    "target = data['Claim']\n",
    "features = data.drop('Claim', axis=1)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345)\n",
    "\n",
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "    \n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=12345)\n",
    "    \n",
    "    return features_downsampled, target_downsampled\n",
    "\n",
    "features_downsampled, target_downsampled = downsample(features_train, target_train, 0.1)\n",
    "\n",
    "# < write code here >\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model.fit(features_downsampled, target_downsampled)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "#print(\"F1:\", f1_score(target_valid, predicted_valid))\n",
    "print(\"F1:\", f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities[:, 1]\n",
    "print(probabilities_one_valid[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "data = pd.read_csv('/datasets/travel_insurance_us_preprocessed.csv')\n",
    "\n",
    "target = data['Claim']\n",
    "features = data.drop('Claim', axis=1)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345)\n",
    "\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model.fit(features_train, target_train)\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "for threshold in np.arange(0, 0.3, 0.02):\n",
    "    predicted_valid = probabilities_one_valid > threshold# < write code here >\n",
    "    precision = precision_score(target_valid, predicted_valid)# < write code here >\n",
    "    recall = recall_score(target_valid, predicted_valid)# < write code here >\n",
    "\n",
    "    print(\"Threshold = {:.2f} | Precision = {:.3f}, Recall = {:.3f}\".format(\n",
    "        threshold, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall curve (PR curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = pd.read_csv('/datasets/travel_insurance_us_preprocessed.csv')\n",
    "\n",
    "target = data['Claim']\n",
    "features = data.drop('Claim', axis=1)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345)\n",
    "\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "precision, recall, thresholds = precision_recall_curve(target_valid, probabilities_valid[:, 1])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.step(recall, precision, where='post')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "Receiver Operating Characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve# < write code here >\n",
    "\n",
    "data = pd.read_csv('/datasets/travel_insurance_us_preprocessed.csv')\n",
    "\n",
    "target = data['Claim']\n",
    "features = data.drop('Claim', axis=1)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345)\n",
    "\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)# < write code here >\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# < plot the graph >\n",
    "plt.plot(fpr, tpr)\n",
    "# ROC curve for random model (looks like a straight line)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "# < use the functions plt.xlim() and plt.ylim() to\n",
    "#   set the boundary for the axes from 0 to 1 >\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "# < use the functions plt.xlabel() and plt.ylabel() to\n",
    "#   name the axes \"False Positive Rate\" and \"True Positive Rate\" >\n",
    "plt.xlabel('False Psitive Rate')\n",
    "plt.ylabel('True Positive Rate') \n",
    "# < add the heading \"ROC curve\" with the function plt.title() >\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC-ROC\n",
    "Area Under Curve ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# < write code here >\n",
    "data = pd.read_csv('/datasets/travel_insurance_us_preprocessed.csv')\n",
    "\n",
    "target = data['Claim']\n",
    "features = data.drop('Claim', axis=1)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345)\n",
    "\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "probabilities_valid = model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "# < write code here >\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "print(auc_roc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
