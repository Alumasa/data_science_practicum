{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and INitializing PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5f50710d7f50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# sc is short for spark context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"IntroToSpark\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# sc is short for spark context\n",
    "sc = SparkContext(appName=\"IntroToSpark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_entry = sc.parallelize(['2009-01-01', 0, 0, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(taxi_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"IntroTo Spark\")# < write code here >\n",
    "weather_entry = sc.parallelize(['2009-01-01', 15.1, 26.1])# < write code here >\n",
    "print(weather_entry.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "APP_NAME = 'sampleApp'\n",
    "\n",
    "spark = SparkSession.builder.appName(APP_NAME).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "APP_NAME = \"DataFrames\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "\n",
    "spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "taxi_df = pd.read_csv('/datasets/pickups_terminal_5.csv')\n",
    "taxi = spark.createDataFrame(taxi_df)\n",
    "\n",
    "print(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Row(date='2009-01-01', hour=0, minute=0, pickups=24.0), \n",
    "Row(date='2009-01-01', hour=0, minute=30, pickups=35.0), \n",
    "Row(date='2009-01-01', hour=1, minute=0, pickups=25.0), \n",
    "Row(date='2009-01-01', hour=1, minute=30, pickups=25.0), \n",
    "Row(date='2009-01-01', hour=2, minute=0, pickups=16.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can break the line using the symbol '\\'\n",
    "spark = SparkSession.builder.appName(APP_NAME) \\\n",
    "        .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format='csv' - specify file format\n",
    "# header='true' - specify that there is a header (column names) in the file\n",
    "# inferSchema='true' - specify that data types should be inferred\n",
    "taxi = spark.read.load('/datasets/pickups_terminal_5.csv', \n",
    "                       format='csv', header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "APP_NAME = \"DataFrames\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "\n",
    "spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "taxi = spark.read.load('/datasets/pickups_terminal_5.csv', format='csv', header='true', inferSchema='true')# < write code here >\n",
    "\n",
    "print(taxi.show(5))# < write code here >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(taxi.count()) #total number of rows in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(taxi[['date', 'hour', 'minute']].show(5)) #susbset columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(taxi.describe().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = taxi.dropna()\n",
    "print(taxi.count())# < write code here >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = taxi.fillna(0)\n",
    "print(taxi.describe().show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA in PySpark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(taxi.summary().show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For future reference*\n",
    "We observe that the pickups data is skewed to the right, since the mean is greater than the median (the row with \"50%\"). We also notice that the maximum value is 310, the mean is 29, and the standard deviation is 22.45. So the maximum is significantly more than three standard deviations from the mean. Therefore, it is clearly an outlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.registerTempTable(\"taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sql(\"SELECT COUNT(*) FROM taxi WHERE pickups > 100\").show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+--------+\n",
    "|count(1)|\n",
    "+--------+\n",
    "|    1431|\n",
    "+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sql(\"SELECT MIN(CAST(date as DATE)), MAX(CAST(date as DATE)) FROM taxi\").show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+-----------------------+-----------------------+\n",
    "|min(CAST(date AS DATE))|max(CAST(date AS DATE))|\n",
    "+-----------------------+-----------------------+\n",
    "|             2009-01-01|             2016-06-30|\n",
    "+-----------------------+-----------------------+\n",
    "\n",
    "#Over seven years!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sql(\"SELECT * FROM taxi ORDER BY pickups DESC\").show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+-------------------+----+------+-------+\n",
    "|               date|hour|minute|pickups|\n",
    "+-------------------+----+------+-------+\n",
    "|2015-11-01 00:00:00|   1|    30|  310.0|\n",
    "|2010-09-23 00:00:00|  22|    30|  288.0|\n",
    "|2012-03-07 00:00:00|  21|     0|  268.0|\n",
    "|2011-03-02 00:00:00|  20|    30|  264.0|\n",
    "|2011-03-02 00:00:00|  18|    30|  263.0|\n",
    "+-------------------+----+------+-------+\n",
    "only showing top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sql(\"SELECT COUNT(DISTINCT date) FROM taxi WHERE pickups > 200\").show())# < write your code here >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+--------------------+\n",
    "|count(DISTINCT date)|\n",
    "+--------------------+\n",
    "|                  21|\n",
    "+--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
