{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.19.2)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/tensorboard/\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\numpy\\\\~ore\\\\_multiarray_tests.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading tensorflow-2.3.1-cp37-cp37m-win_amd64.whl (342.5 MB)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp37-cp37m-win_amd64.whl (12.7 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.33.2-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (50.3.0.post20201103)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=26f1514b27e5276ed152073afc0a71bbc095506e29131947f57173962d6b310e\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\3f\\e3\\ec\\8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built termcolor\n",
      "Installing collected packages: tensorboard-plugin-wit, absl-py, markdown, grpcio, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, numpy, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, gast, google-pasta, opt-einsum, tensorflow-estimator, termcolor, keras-preprocessing, astunparse, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (2.3.1)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from protobuf>=3.9.2->tensorflow) (50.3.0.post20201103)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-64156d691fe5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9c5e0a19b646>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     raise ImportError(\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some data\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('/datasets/train_data_n.csv')\n",
    "features = data.drop('target', axis=1)\n",
    "target = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in sklearn\n",
    "# import from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# train the model\n",
    "model.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in keras\n",
    "# import Keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# create the model\n",
    "model = keras.models.Sequential()\n",
    "# indicate how the neural network is arranged\n",
    "model.add(keras.layers.Dense(units=1, input_dim=features.shape[1]))\n",
    "# indicate how the neural network is trained\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "# train the model\n",
    "model.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's set the model class to Sequential. \n",
    "- This class is used for models with sequential layers. Layer is a set of neurons that share the same input and output.\n",
    "\n",
    "- The keras.layers.Dense() command creates one layer of neurons. \n",
    "- \"Dense\" means that every input will be connected to every neuron, or output. \n",
    "- The units parameter sets the number of neurons in the layer, and input_dim sets the number of inputs in the layer. \n",
    "- Note that this parameter doesn't take the bias into account.\n",
    "- To add the fully connected layer to the model, call the model.add() method:\n",
    "- `model.compile` - It prepares the model for training. \n",
    "- Specify MSE as the regression task loss function for the loss parameter. \n",
    "-Set the gradient descent method for the optimizer='sgd' parameter. Remember, we'll be training neural networks using SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Layers where all inputs are connected to all neurons are called **fully connected layers**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.layers.Dense(units=2, input_dim=3)` - two neurons and three inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "data = pd.read_csv('/datasets/train_data_n.csv')\n",
    "features = data.drop('target', axis=1)\n",
    "target = data['target']\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(units=1, input_dim=features.shape[1]))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "model.fit(features, target, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = how many times has our data passed through the algorithm) to get the MSE to be less than 6.55.\n",
    "model.fit(features, target, verbose=2, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the loss function value for the validation set.\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "data_train = pd.read_csv('/datasets/train_data_n.csv')\n",
    "features_train = data_train.drop('target', axis=1)\n",
    "target_train = data_train['target']\n",
    "\n",
    "data_valid = pd.read_csv('/datasets/test_data_n.csv')\n",
    "features_valid = data_valid.drop('target', axis=1)\n",
    "target_valid = data_valid['target']\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(units=1, input_dim=features_train.shape[1]))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "model.fit(features_train, target_train, validation_data=(features_valid, target_valid), epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the activation function to the fully connected layer:\n",
    "keras.layers.Dense(units=1, input_dim=features_train.shape[1], activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the loss function from MSE to binary_crossentropy;\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "df = pd.read_csv('/datasets/train_data_n.csv')\n",
    "df['target'] = (df['target'] > df['target'].median()).astype(int)\n",
    "features_train = df.drop('target', axis=1)\n",
    "target_train = df['target']\n",
    "\n",
    "df_val = pd.read_csv('/datasets/test_data_n.csv')\n",
    "df_val['target'] = (df_val['target'] > df['target'].median()).astype(int)\n",
    "features_valid = df_val.drop('target', axis=1)\n",
    "target_valid = df_val['target']\n",
    "\n",
    "# < write code here >\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(units=1, input_dim=features_train.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd')\n",
    "model.fit(features_train, target_train, validation_data=(features_valid, target_valid), verbose=2, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model's accuracy using validation data\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('/datasets/train_data_n.csv')\n",
    "df['target'] = (df['target'] > df['target'].median()).astype(int)\n",
    "features_train = df.drop('target', axis=1)\n",
    "target_train = df['target']\n",
    "\n",
    "df_val = pd.read_csv('/datasets/test_data_n.csv')\n",
    "df_val['target'] = (df_val['target'] > df['target'].median()).astype(int)\n",
    "features_valid = df_val.drop('target', axis=1)\n",
    "target_valid = df_val['target']\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(units=1, input_dim=features_train.shape[1], \n",
    "                             activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd')\n",
    "model.fit(features_train, target_train, epochs=5, verbose=0,\n",
    "          validation_data=(features_valid, target_valid))\n",
    "\n",
    "\n",
    "# < write code here >\n",
    "predictions = model.predict(features_valid) > 0.5\n",
    "#print(predictions)\n",
    "score = accuracy_score(predictions, target_valid)\n",
    "print(\"Accuracy:\", score)# < write code here >)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trace quality at each epoch\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('/datasets/train_data_n.csv')\n",
    "df['target'] = (df['target'] > df['target'].median()).astype(int)\n",
    "features_train = df.drop('target', axis=1)\n",
    "target_train = df['target']\n",
    "\n",
    "df_val = pd.read_csv('/datasets/test_data_n.csv')\n",
    "df_val['target'] = (df_val['target'] > df['target'].median()).astype(int)\n",
    "features_valid = df_val.drop('target', axis=1)\n",
    "target_valid = df_val['target']\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(units=1, input_dim=features_train.shape[1], \n",
    "                             activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "model.fit(features_train, target_train, epochs=10, verbose=2,\n",
    "          validation_data=(features_valid, target_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Neural Networks in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('/datasets/train_data_n.csv')\n",
    "df['target'] = (df['target'] > df['target'].median()).astype(int)\n",
    "features_train = df.drop('target', axis=1)\n",
    "target_train = df['target']\n",
    "\n",
    "df_val = pd.read_csv('/datasets/test_data_n.csv')\n",
    "df_val['target'] = (df_val['target'] > df['target'].median()).astype(int)\n",
    "features_valid = df_val.drop('target', axis=1)\n",
    "target_valid = df_val['target']\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "# < write code here >\n",
    "#adding hidden layers with different neurons\n",
    "model.add(keras.layers.Dense(units=10, activation='sigmoid',input_dim=features_train.shape[1]))\n",
    "model.add(keras.layers.Dense(units=1, activation='sigmoid', input_dim=features_train.shape[1]))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "\n",
    "model.fit(features_train, target_train, epochs=10, verbose=2,\n",
    "          validation_data=(features_valid, target_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Images in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/datasets/ds_cv_images/face.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-694a3a7b3405>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/datasets/ds_cv_images/face.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2808\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2809\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2810\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datasets/ds_cv_images/face.png'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open('/datasets/ds_cv_images/face.png')\n",
    "array = np.array(image)\n",
    "print(array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we get a 2-D array\n",
    "[[255 255 255 255 237 217 239 255 255 255 255 255 255]\n",
    " [255 255 190  75  29  29  30  81 198 255 255 255 255]\n",
    " [255 147  30  29  29  29  29  29  31 160 255 255 255]\n",
    " [185  29  29  29  29  29  29  29  29  31 198 255 255]\n",
    " [ 61  29  29  29  29  29  29  29  29  29  74 255 255]\n",
    " [108 121 121 121 121 121 121 121 121 121 102 219 255]\n",
    " [250 255 255 255 255 255 255 255 255 255 238 107 168]\n",
    " [255 238 153 150 152 244 201 152 150 178 253 103 144]\n",
    " [248 243 121 108 114 225 184 130 112 154 235 103  62]\n",
    " [197 255 227 168 231 149 230 196 179 251 183  29  29]\n",
    " [105 255 255 219 195 191 184 195 235 255  91  29  29]\n",
    " [ 30 187 255 234 218 218 218 218 243 174  29  29  29]\n",
    " [ 29  38 180 255 255 255 255 255 169  35  29  29  29]\n",
    " [ 29  29  29  82 153 174 150  76  29  29  29  29  29]\n",
    " [ 29  29  29  29  29  29  29  29  29  29  29  29  29]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = Image.open('/datasets/ds_cv_images/face.png')\n",
    "array = np.array(image)\n",
    "\n",
    "# < write code here >\n",
    "plt.imshow(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set color to balck and white\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = Image.open('/datasets/ds_cv_images/face.png')\n",
    "array = np.array(image)\n",
    "\n",
    "plt.imshow(array, cmap='gray')# < write code here >)\n",
    "# < write code here >\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repaint corners\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = Image.open('/datasets/ds_cv_images/face.png')\n",
    "array = np.array(image)\n",
    "\n",
    "# < write code here >\n",
    "#print(array.shape)\n",
    "array[0][0] = 0\n",
    "array[14][12] = 255\n",
    "#print(array)\n",
    "\n",
    "plt.imshow(array, cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the scale\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = Image.open('/datasets/ds_cv_images/face.png')\n",
    "array = np.array(image)\n",
    "\n",
    "# < write code here >\n",
    "array = array / 255\n",
    "#print(array)\n",
    "\n",
    "plt.imshow(array, cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 255],\n",
       "       [255,   0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[0, 255],\n",
    "                    [255, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0, 255,   0],\n",
       "        [128,   0, 255]],\n",
       "\n",
       "       [[ 12,  89,   0],\n",
       "        [  5,  89, 245]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[[0, 255, 0], [128, 0, 255]], \n",
    "                    [[12, 89, 0], [5,  89, 245]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = Image.open('/datasets/ds_cv_images/cat.jpg')\n",
    "array = np.array(image)\n",
    "\n",
    "# < write code here >\n",
    "print(array.shape)\n",
    "#print(array)\n",
    "#print(array.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output the red channel only\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = Image.open('/datasets/ds_cv_images/cat.jpg')\n",
    "array = np.array(image)\n",
    "\n",
    "red_channel =array[:,:,0] \n",
    "# < write code here >\n",
    "\n",
    "plt.imshow(red_channel)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification\n",
    "\n",
    "-  We need to build a ten-class classifier for the clothing store aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "features_train = np.load('/datasets/fashion_mnist/train_features.npy')\n",
    "target_train = np.load('/datasets/fashion_mnist/train_target.npy')\n",
    "features_test = np.load('/datasets/fashion_mnist/test_features.npy')\n",
    "target_test = np.load('/datasets/fashion_mnist/test_target.npy')\n",
    "\n",
    "print(\"Train:\", features_train.shape)# < write code here >)\n",
    "print(\"Test:\", features_test.shape)# < write code here >)\n",
    "\n",
    "print(\"First image class:\", target_train[0])# < write code here >)\n",
    "plt.imshow(features_train[0], cmap='gray')# < write code here >)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis = [[0, 255, 0], [128, 0, 255]]\n",
    "a_lis = np.array(lis)\n",
    "type(a_lis)\n",
    "a_lis.shape\n",
    "len(a_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[0, 255, 0], [128, 0, 255]], \n",
    "                    [[12, 89, 0], [5,  89, 245]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.reshape(x, (3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "features_train = np.load('/datasets/fashion_mnist/train_features.npy')\n",
    "target_train = np.load('/datasets/fashion_mnist/train_target.npy')\n",
    "features_test = np.load('/datasets/fashion_mnist/test_features.npy')\n",
    "target_test = np.load('/datasets/fashion_mnist/test_target.npy')\n",
    "\n",
    "# < write code here >\n",
    "features_train = features_train.reshape(features_train.shape[0], 28 * 28)\n",
    "features_test = features_test.reshape(features_test.shape[0], 28 * 28)\n",
    "\n",
    "print(\"Train:\", features_train.shape)\n",
    "print(\"Test:\", features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train a neural network. First, create a logistic regression with ten classes in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "features_train = np.load('/datasets/fashion_mnist/train_features.npy')\n",
    "target_train = np.load('/datasets/fashion_mnist/train_target.npy')\n",
    "features_test = np.load('/datasets/fashion_mnist/test_features.npy')\n",
    "target_test = np.load('/datasets/fashion_mnist/test_target.npy')\n",
    "\n",
    "features_train = features_train.reshape(features_train.shape[0], 28 * 28)\n",
    "features_test = features_test.reshape(features_test.shape[0], 28 * 28)\n",
    "\n",
    "model = keras.models.Sequential()# < write code here >\n",
    "model.add(keras.layers.Dense(units=10, activation='softmax', input_dim=28*28))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "model.fit(features_train, target_train, epochs=1, verbose=2,\n",
    "          validation_data=(features_test, target_test))\n",
    "\n",
    "# < write code here >\n",
    "\n",
    "#model.fit(# < write code here >)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to train a model on the server\n",
    "#pretest the code\n",
    "#1. loading the training sample\n",
    "def load_train(path):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(path):\n",
    "    features_train = np.load(path + 'train_features.npy')\n",
    "    target_train = np.load(path + 'train_target.npy')\n",
    "    features_train = features_train.reshape(features_train.shape[0], 28 * 28) / 255.\n",
    "    return features_train, target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a model for a neural network and use the return command to return it\n",
    "#Use the input_shape parameter to specify the data size in the first layer.\n",
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_shape=input_shape, activation='softmax'))\n",
    "    model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launch the model\n",
    "def train_model(model, train_data, test_data, batch_size, epochs,\n",
    "                steps_per_epoch, validation_steps):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, test_data, batch_size=32, epochs=5,\n",
    "                steps_per_epoch=None, validation_steps=None):\n",
    "\n",
    "    features_train, target_train = train_data\n",
    "    features_test, target_test = test_data\n",
    "    model.fit(features_train, target_train, \n",
    "              validation_data=(features_test, target_test),\n",
    "              batch_size=batch_size, epochs=epochs,\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              validation_steps=validation_steps,\n",
    "              verbose=2, shuffle=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at the entire code for logistic regression:\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "def load_train(path):\n",
    "    features_train = np.load(path + 'train_features.npy')\n",
    "    target_train = np.load(path + 'train_target.npy')\n",
    "    features_train = features_train.reshape(features_train.shape[0], 28 * 28) / 255.\n",
    "    return features_train, target_train\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_shape=input_shape, activation='softmax'))\n",
    "    model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_data, test_data, batch_size=32, epochs=5,\n",
    "               steps_per_epoch=None, validation_steps=None):\n",
    "\n",
    "    features_train, target_train = train_data\n",
    "    features_test, target_test = test_data\n",
    "    model.fit(features_train, target_train, \n",
    "              validation_data=(features_test, target_test),\n",
    "              batch_size=batch_size, epochs=epochs,\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              validation_steps=validation_steps,\n",
    "              verbose=2, shuffle=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-dimensional convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's express a one-dimensional convolution in Python\n",
    "def convolve(sequence, weights):\n",
    "    convolution = np.zeros(len(sequence) - len(weights) + 1)\n",
    "    for i in range(convolution.shape[0]):\n",
    "        convolution[i] = np.sum(np.array(weights) * np.array(sequence[i:i + len(weights)]))\n",
    "    return convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [2, 3, 5, 7, 11]\n",
    "w = [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 2., 4.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolve(s, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolution = np.zeros(len(s) - len(w) + 1)\n",
    "convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolution.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(s[1:1 + len(w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-dimensional Convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [[1, 1, 1, 0, 0],\n",
    "     [0, 1, 1, 1, 0],\n",
    "     [0, 0, 1, 1, 1],\n",
    "     [0, 0, 1, 1, 0],\n",
    "     [0, 1, 1, 0, 0]]\n",
    "\n",
    "w = [[1, 0, 1],\n",
    "     [0, 1, 0],\n",
    "     [1, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [[0, -3], [7, 13]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Conv2D(filters, kernel_size, strides, padding, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's what all the parameters mean:\n",
    "    - **filters**: The number of filters, which corresponds to the size of the output tensor.\n",
    "    - **kernel_size**: The spatial size of the filter K. Any filter is a tensor with the size K×K×D, where D is equal to the depth of the input image.\n",
    "    - **strides**: A stride determines how far the filter shifts over the input matrix. It's set to 1 by default.\n",
    "    - **padding**: This parameter determines the width of the zero padding. There are two types of padding: valid and same. The default type of padding is valid and is equal to zero. Same sets the size of the padding automatically so that the width and height of the output tensor is equal to the width and height of the input tensor.\n",
    "    - **activation**: This function is applied immediately after the convolution. You can use the activation functions already familiar to you: 'relu' and 'sigmoid'. By default, this parameter is None, i.e. activation is disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# this tensor has a size of (None, 32, 32, 3)\n",
    "# the first dimension defines different objects\n",
    "# it's set to None because the size of the batch is unknown\n",
    "\n",
    "model.add(Conv2D(filters=4, kernel_size=(3, 3), input_shape=(32, 32, 3)))\n",
    "\n",
    "# this tensor has a size of (None, 30, 30, 4)\n",
    "\n",
    "model.add(Flatten()) #makes the multidimensional into a one dimensional\n",
    "\n",
    "# this tensor has a size of (None, 3600)\n",
    "# where 3600 = 30 * 30 * 4\n",
    "\n",
    "model.add(Dense(...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "features_train = np.load('/datasets/fashion_mnist/train_features.npy')\n",
    "target_train = np.load('/datasets/fashion_mnist/train_target.npy')\n",
    "features_test = np.load('/datasets/fashion_mnist/test_features.npy')\n",
    "target_test = np.load('/datasets/fashion_mnist/test_target.npy')\n",
    "\n",
    "# Change the image's dimensions and set it to a range of [0, 1].\n",
    "# While changing the size, you can set one of the dimensions to -1, \n",
    "# so that it is calculated automatically\n",
    "features_train = features_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "features_test = features_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "\n",
    "model = Sequential()\n",
    "# < write code here >\n",
    "model.add(Conv2D(filters=4, kernel_size=(3, 3), input_shape=(28, 28, 1)))\n",
    "model.add(Flatten())          \n",
    "model.add(Dense(10, activation='softmax'))         \n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- conv2D: (None, 26, 26, 4), 40parameters\n",
    "- flatten (None, 2704), 0params\n",
    "- dense (None, 10), 27050params\n",
    "    - total params; 27,090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "features_train = np.load('/datasets/fashion_mnist/train_features.npy')\n",
    "target_train = np.load('/datasets/fashion_mnist/train_target.npy')\n",
    "features_test = np.load('/datasets/fashion_mnist/test_features.npy')\n",
    "target_test = np.load('/datasets/fashion_mnist/test_target.npy')\n",
    "features_train = features_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "features_test = features_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=4, kernel_size=(3, 3), padding='same',activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(filters=4, kernel_size=(3, 3), strides=2, padding='same',activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "model.summary()\n",
    "model.fit(features_train, target_train, epochs=1, verbose=1,\n",
    "steps_per_epoch=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average pooling in Keras:\n",
    "keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a classic LeNet architecture:\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "features_train = np.load('/datasets/fashion_mnist/train_features.npy')\n",
    "target_train = np.load('/datasets/fashion_mnist/train_target.npy')\n",
    "features_test = np.load('/datasets/fashion_mnist/test_features.npy')\n",
    "target_test = np.load('/datasets/fashion_mnist/test_target.npy')\n",
    "\n",
    "features_train = features_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "features_test = features_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=4, kernel_size=(3, 3), padding='same',\n",
    "                 activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(filters=4, kernel_size=(3, 3), strides=2, padding='same',\n",
    "                 activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "model.summary()\n",
    "model.fit(features_train, target_train, epochs=1, verbose=1,\n",
    "          steps_per_epoch=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, AvgPool2D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "features_train = np.load('/datasets/fashion_mnist/train_features.npy')\n",
    "target_train = np.load('/datasets/fashion_mnist/train_target.npy')\n",
    "features_test = np.load('/datasets/fashion_mnist/test_features.npy')\n",
    "target_test = np.load('/datasets/fashion_mnist/test_target.npy')\n",
    "\n",
    "\n",
    "features_train = features_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "features_test = features_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "\n",
    "# < write code here >\n",
    "model = Sequential()\n",
    "model.add(Conv2D(6, (5, 5), padding='same', activation='tanh',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(AvgPool2D(pool_size=(2, 2)))\n",
    " \n",
    "model.add(Conv2D(16, (5, 5), padding='valid', activation='tanh'))\n",
    "model.add(AvgPool2D(pool_size=(2, 2)))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='tanh'))\n",
    " \n",
    "model.add(Dense(84, activation='tanh'))\n",
    " \n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "model.summary()\n",
    "model.fit(features_train, target_train, epochs=1, verbose=1,\n",
    "          steps_per_epoch=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Adam Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the algorithm class to configure the hyperparameters:\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate:\n",
    "optimizer = Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
