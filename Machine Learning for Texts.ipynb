{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer  = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heaven'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('heavens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"All models are wrong, but some are useful.\"\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'model', 'are', 'wrong', ',', 'but', 'some', 'are', 'useful', '.']\n"
     ]
    }
   ],
   "source": [
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all model are wrong , but some are useful .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-67741b070562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parser'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ner'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "doc = nlp(text.lower())\n",
    "\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization using spaCy\n",
    "import pandas as pd\n",
    "import random             # in order to select a random review\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small.tsv', sep='\\t')\n",
    "corpus = data['review']\n",
    "\n",
    "def lemmatize(text):\n",
    "\n",
    "    # < write code here >\n",
    "    text = nlp(text.lower())\n",
    "    lemmas = [token.lemma_ for token in text]\n",
    "    text_lemmas = \" \".join(lemmas)\n",
    "    return text_lemmas\n",
    "\n",
    "# store the review index in the review_idx variable\n",
    "# either as a random number or a fixed value, e.g. 2557 \n",
    "#review_idx = random.randint(0, len(corpus)-1)\n",
    "review_idx = 2557\n",
    "\n",
    "review = corpus[review_idx]\n",
    "\n",
    "print(\"The original text:\", review)\n",
    "print()\n",
    "print(\"The lemmatized text:\", lemmatize(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pattern' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-978b5ccfa074>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# substitution — what each pattern match should be substituted with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# text — the text which the function scans for pattern matches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pattern' is not defined"
     ]
    }
   ],
   "source": [
    "# pattern\n",
    "# substitution — what each pattern match should be substituted with\n",
    "# text — the text which the function scans for pattern matches\n",
    "re.sub(pattern, substitution, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "\n",
      "Hello!\\n\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello!\\n\")\n",
    "\n",
    "print(r\"Hello!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a range of letters is indicated by a hyphen:\n",
    "# a-z = abcdefghijklmnopqrstuvwxyz\n",
    "r\"[a-zA-Z]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find apostrophes as well\n",
    "r\"[a-zA-Z']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review text\n",
    "text = \"\"\"\n",
    "I liked this show from the first episode I saw, which was the \"Rhapsody in Blue\" episode (for those that don't know what that is, the Zan going insane and becoming pau lvl 10 ep). Best visuals and special effects I've seen on a television series, nothing like it anywhere.\n",
    "\"\"\"\n",
    "text_sub = re.sub(r\"[^a-zA-Z']\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = text_sub.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I liked this show from the first episode I saw which was the Rhapsody in Blue episode for those that don't know what that is the Zan going insane and becoming pau lvl ep Best visuals and special effects I've seen on a television series nothing like it anywhere\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'liked', 'this', 'show']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"            I   liked   this   show   \"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I liked this show'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(['I', 'liked', 'this', 'show'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random             # in order to select a random review\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small.tsv', sep='\\t')\n",
    "corpus = data['review']\n",
    "\n",
    "def clear_text(text):\n",
    "    \n",
    "    # < write code here >\n",
    "    text = re.sub(r\"[^a-zA-Z']\", \" \", text)\n",
    "    text = text.split()\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        lemmas.append(token.lemma_)\n",
    "        \n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# store the review index in the review_idx variable\n",
    "# either as a random number or a fixed value, e.g. 2557 \n",
    "review_idx = random.randint(0, len(corpus)-1)\n",
    "# review_idx = 2557\n",
    "\n",
    "review = corpus[review_idx]\n",
    "\n",
    "print(\"The original text:\", review)\n",
    "print()\n",
    "print(\"The lemmatized text:\", lemmatize(clear_text(review)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "text = \"\"\"For want of a nail the shoe was lost. For want of a shoe the horse was lost. For want of a horse the rider was lost.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.lemma_ for token in doc if not token.is_punct]\n",
    "\n",
    "bow = Counter(tokens)\n",
    "\n",
    "vector = [bow[token] for token in sorted(bow)]\n",
    "\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "    Natural-language processing (NLP) is an area of\n",
    "    computer science and artificial intelligence\n",
    "    concerned with the interactions between computers\n",
    "    and human (natural) languages.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n natural language processing nlp',\n",
       " 'natural language processing nlp is',\n",
       " 'language processing nlp is an',\n",
       " 'processing nlp is an area',\n",
       " 'nlp is an area of\\n',\n",
       " 'is an area of\\n computer',\n",
       " 'an area of\\n computer science',\n",
       " 'area of\\n computer science and',\n",
       " 'of\\n computer science and artificial',\n",
       " 'computer science and artificial intelligence\\n',\n",
       " 'science and artificial intelligence\\n concerned',\n",
       " 'and artificial intelligence\\n concerned with',\n",
       " 'artificial intelligence\\n concerned with the',\n",
       " 'intelligence\\n concerned with the interactions',\n",
       " 'concerned with the interactions between',\n",
       " 'with the interactions between computers\\n',\n",
       " 'the interactions between computers\\n and',\n",
       " 'interactions between computers\\n and human',\n",
       " 'between computers\\n and human natural',\n",
       " 'computers\\n and human natural languages',\n",
       " 'and human natural languages \\n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ngrams(s, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', 'five']\n",
      "[['one', 'two', 'three', 'four', 'five'], ['two', 'three', 'four', 'five'], ['three', 'four', 'five']]\n",
      "[('one', 'two', 'three'), ('two', 'three', 'four'), ('three', 'four', 'five')]\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "s = \"one two three four five\"\n",
    "\n",
    "tokens = s.split(\" \")\n",
    "# tokens = [\"one\", \"two\", \"three\", \"four\", \"five\"]\n",
    "print(tokens)\n",
    "\n",
    "sequences = [tokens[i:] for i in range(3)]\n",
    "# The above will generate sequences of tokens starting\n",
    "# from different elements of the list of tokens.\n",
    "# The parameter in the range() function controls\n",
    "# how many sequences to generate.\n",
    "#\n",
    "# sequences = [\n",
    "#   ['one', 'two', 'three', 'four', 'five'],\n",
    "#   ['two', 'three', 'four', 'five'],\n",
    "#   ['three', 'four', 'five']]\n",
    "print(sequences)\n",
    "\n",
    "bigrams = zip(*sequences)\n",
    "print(list(bigrams))\n",
    "# The zip function takes the sequences as a list of inputs\n",
    "# (using the * operator, this is equivalent to\n",
    "# zip(sequences[0], sequences[1], sequences[2]).\n",
    "# Each tuple it returns will contain one element from\n",
    "# each of the sequences.\n",
    "# \n",
    "# To inspect the content of bigrams, try:\n",
    "# print(list(bigrams))\n",
    "# which will give the following:\n",
    "#\n",
    "# [\n",
    "#   ('one', 'two', 'three'),\n",
    "#   ('two', 'three', 'four'),\n",
    "#   ('three', 'four', 'five')\n",
    "# ]\n",
    "#\n",
    "# Note: even though the first sequence has 5 elements,\n",
    "# zip will stop after returning 3 tuples, because the\n",
    "# last sequence only has 3 elements. In other words,\n",
    "# the zip function automatically handles the ending of\n",
    "# the n-gram generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "s = s.lower()\n",
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "output = list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 'two', 'three'), ('two', 'three', 'four'), ('three', 'four', 'five')]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'for want of a nail the shoe be lose',\n",
    "    'for want of a shoe the horse be lose',\n",
    "    'for want of a horse the rider be lose',\n",
    "    'for want of a rider the message be lose',\n",
    "    'for want of a message the battle be lose',\n",
    "    'for want of a battle the kingdom be lose',\n",
    "    'and all for the want of a horseshoe nail'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The counter extracts unique words from the corpus and counts how many times they appear in each text of the corpus. \n",
    "#The counter doesn't count separate letters.\n",
    "# bow = bag of words\n",
    "bow = count_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 16)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape\n",
    "#16 unique words, 7 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1]\n",
      " [0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1]\n",
      " [0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1]\n",
      " [0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1]\n",
      " [1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'and',\n",
       " 'battle',\n",
       " 'be',\n",
       " 'for',\n",
       " 'horse',\n",
       " 'horseshoe',\n",
       " 'kingdom',\n",
       " 'lose',\n",
       " 'message',\n",
       " 'nail',\n",
       " 'of',\n",
       " 'rider',\n",
       " 'shoe',\n",
       " 'the',\n",
       " 'want']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The list of unique words in the bag is called a vocabulary. \n",
    "#It's stored in the counter and can be accessed by calling the get_feature_names() method:\n",
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams\n",
    "count_vect = CountVectorizer(ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2 = count_vect.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 1 0 1 1]\n",
      " [0 1 0 0 1 0 0 0 1 1]\n",
      " [0 1 0 0 1 0 0 1 0 1]\n",
      " [0 0 0 0 1 1 0 1 0 1]\n",
      " [1 0 0 0 1 1 0 0 0 1]\n",
      " [1 0 0 1 1 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['battle',\n",
       " 'horse',\n",
       " 'horseshoe',\n",
       " 'kingdom',\n",
       " 'lose',\n",
       " 'message',\n",
       " 'nail',\n",
       " 'rider',\n",
       " 'shoe',\n",
       " 'want']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# < write code here >\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small_lemm.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "# create a bag-of-words without checking for stop words\n",
    "# < write code here >\n",
    "count_vect = CountVectorizer()\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "print(\"The BoW size with stop words:\", bow.shape)\n",
    "\n",
    "# create a bag-of-words with checking for stop words\n",
    "# < write code here >\n",
    "stop_words = set(stopwords.words('english'))\n",
    "count_vect = CountVectorizer(stop_words=stop_words)\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(\"The BoW size without stop words:\", bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small_lemm.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "# create an n-gram with n=2 and store it in the n_gram variable\n",
    "\n",
    "# < write code here >\n",
    "count_vect = CountVectorizer(ngram_range=(2, 2))\n",
    "n_gram = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(\"The size of 2-gram:\", n_gram.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = count_tf_idf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "# import TfidfVectorizer\n",
    "# < write code here >\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small_lemm.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "stop_words = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "tf_idf = count_tf_idf.fit_transform(corpus)\n",
    "# < write code here >\n",
    "\n",
    "print(\"The TF-IDF matrix size:\", tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_valid = accuracy_score(pred, target_valid)\n",
    "print(accuracy_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       i see this movie last year in medium class and...\n",
       "1       i must admit there be few book with correspond...\n",
       "2       i think that the shot and light be very poor w...\n",
       "3       a few week ago i read the classic george orwel...\n",
       "4       i see this movie literally directly after fini...\n",
       "                              ...                        \n",
       "2022    director douglas sirk score again with this th...\n",
       "2023    spoiler spoiler release in and consider quite ...\n",
       "2024    fabulous film rent the dvd recently and be flo...\n",
       "2025    rich alcoholic robert stack fall in love with ...\n",
       "2026    director douglas sirk once say ` there be a ve...\n",
       "Name: review_lemm, Length: 2027, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "# import TfidfVectorizer\n",
    "# < write code here >\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = pd.read_csv('datasets/imdb_reviews_small_lemm_train.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TF-IDF matrix size: (2027, 18036)\n"
     ]
    }
   ],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "tf_idf = count_tf_idf.fit_transform(corpus)\n",
    "\n",
    "print(\"The TF-IDF matrix size:\", tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2027x18036 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 196664 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tconst', 'original_title', 'review', 'review_lemm', 'pos'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "2022    1\n",
       "2023    1\n",
       "2024    1\n",
       "2025    1\n",
       "2026    1\n",
       "Name: pos, Length: 2027, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = data['pos']\n",
    "#features = corpus\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    tf_idf, target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1621x18036 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 159016 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1254    0\n",
       "898     0\n",
       "1187    0\n",
       "37      1\n",
       "1200    0\n",
       "       ..\n",
       "1359    1\n",
       "516     0\n",
       "122     0\n",
       "641     1\n",
       "457     1\n",
       "Name: pos, Length: 406, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0, solver='liblinear')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, solver='liblinear')\n",
    "model.fit(tf_idf, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('datasets/imdb_reviews_small_lemm_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = test_set['review_lemm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TF-IDF matrix size: (2220, 18036)\n"
     ]
    }
   ],
   "source": [
    "test_tf_idf = count_tf_idf.transform(test_target)\n",
    "\n",
    "print(\"The TF-IDF matrix size:\", test_tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2220,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['pos'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>original_title</th>\n",
       "      <th>review</th>\n",
       "      <th>review_lemm</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0108999</td>\n",
       "      <td>...And the Earth Did Not Swallow Him</td>\n",
       "      <td>I rented this movie from a local library witho...</td>\n",
       "      <td>i rent this movie from a local library without...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0108999</td>\n",
       "      <td>...And the Earth Did Not Swallow Him</td>\n",
       "      <td>The movie \". . . And The Earth Did not Swallow...</td>\n",
       "      <td>the movie and the earth do not swallow -PRON- ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0108999</td>\n",
       "      <td>...And the Earth Did Not Swallow Him</td>\n",
       "      <td>I was very moved by the young life experiences...</td>\n",
       "      <td>i be very move by the young life experience of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0108999</td>\n",
       "      <td>...And the Earth Did Not Swallow Him</td>\n",
       "      <td>Recently finally available in DVD (11/11/08), ...</td>\n",
       "      <td>recently finally available in dvd severo p rez...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0063308</td>\n",
       "      <td>Un minuto per pregare, un istante per morire</td>\n",
       "      <td>I saw this movie over 20 years ago and had rat...</td>\n",
       "      <td>i see this movie over year ago and have rather...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>tt0472278</td>\n",
       "      <td>Vampire Assassin</td>\n",
       "      <td>Ron Hall pulls a triple threat as he writes, d...</td>\n",
       "      <td>ron hall pull a triple threat as -PRON- write ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>tt0832971</td>\n",
       "      <td>Vanaja</td>\n",
       "      <td>Winning 26 out of the 28 awards it was nominat...</td>\n",
       "      <td>win out of the award -PRON- be nominate for th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>tt0832971</td>\n",
       "      <td>Vanaja</td>\n",
       "      <td>Vanaja is a film of superlatives. It has an ex...</td>\n",
       "      <td>vanaja be a film of superlative -PRON- have an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>tt0832971</td>\n",
       "      <td>Vanaja</td>\n",
       "      <td>This is not your typical Indian film. There is...</td>\n",
       "      <td>this be not -PRON- typical indian film there b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>tt0832971</td>\n",
       "      <td>Vanaja</td>\n",
       "      <td>Vanaja (2006), written and directed by Rajnesh...</td>\n",
       "      <td>vanaja write and direct by rajnesh domalpalli ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2220 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tconst                                original_title  \\\n",
       "0     tt0108999          ...And the Earth Did Not Swallow Him   \n",
       "1     tt0108999          ...And the Earth Did Not Swallow Him   \n",
       "2     tt0108999          ...And the Earth Did Not Swallow Him   \n",
       "3     tt0108999          ...And the Earth Did Not Swallow Him   \n",
       "4     tt0063308  Un minuto per pregare, un istante per morire   \n",
       "...         ...                                           ...   \n",
       "2215  tt0472278                              Vampire Assassin   \n",
       "2216  tt0832971                                        Vanaja   \n",
       "2217  tt0832971                                        Vanaja   \n",
       "2218  tt0832971                                        Vanaja   \n",
       "2219  tt0832971                                        Vanaja   \n",
       "\n",
       "                                                 review  \\\n",
       "0     I rented this movie from a local library witho...   \n",
       "1     The movie \". . . And The Earth Did not Swallow...   \n",
       "2     I was very moved by the young life experiences...   \n",
       "3     Recently finally available in DVD (11/11/08), ...   \n",
       "4     I saw this movie over 20 years ago and had rat...   \n",
       "...                                                 ...   \n",
       "2215  Ron Hall pulls a triple threat as he writes, d...   \n",
       "2216  Winning 26 out of the 28 awards it was nominat...   \n",
       "2217  Vanaja is a film of superlatives. It has an ex...   \n",
       "2218  This is not your typical Indian film. There is...   \n",
       "2219  Vanaja (2006), written and directed by Rajnesh...   \n",
       "\n",
       "                                            review_lemm  pos  \n",
       "0     i rent this movie from a local library without...    0  \n",
       "1     the movie and the earth do not swallow -PRON- ...    1  \n",
       "2     i be very move by the young life experience of...    1  \n",
       "3     recently finally available in dvd severo p rez...    1  \n",
       "4     i see this movie over year ago and have rather...    0  \n",
       "...                                                 ...  ...  \n",
       "2215  ron hall pull a triple threat as -PRON- write ...    0  \n",
       "2216  win out of the award -PRON- be nominate for th...    1  \n",
       "2217  vanaja be a film of superlative -PRON- have an...    1  \n",
       "2218  this be not -PRON- typical indian film there b...    1  \n",
       "2219  vanaja write and direct by rajnesh domalpalli ...    1  \n",
       "\n",
       "[2220 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_csv('predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9224f0faab3d4466bb5f60046eaed700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize the tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2009, 2003, 2200, 18801, 2000, 2224, 19081, 102]\n"
     ]
    }
   ],
   "source": [
    "#Convert the text into IDs of tokens, and the BERT tokenizer will return IDs of tokens rather than tokens\n",
    "example = 'It is very handy to use transformers'\n",
    "ids = tokenizer.encode(example, add_special_tokens=True)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  2009  2003  2200 18801  2000  2224 19081   102     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "#BERT accepts vectors of a fixed length, e.g. of 512 tokens. \n",
    "n = 512\n",
    "\n",
    "padded = np.array(ids[:n] + [0]*(n - len(ids)))\n",
    "\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "#create a mask for the important tokens, indicating zero and non-zero values\n",
    "#zeros do not carry significant information\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "data = pd.read_csv('datasets/imdb_reviews_small_lemm_train.tsv', sep='\\t')\n",
    "\n",
    "# initializing tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# texts to tokens\n",
    "text = 'It is very handy to use transformers'\n",
    "ids = tokenizer.encode(text.lower(), add_special_tokens=True)\n",
    "ids = data['review'].apply(\n",
    " lambda x: tokenizer.encode(x.lower(), \n",
    "                             add_special_tokens=True, truncation=True, \n",
    "                             max_length=512))\n",
    "\n",
    "# padding (appending zero's to the vector to make its length equal to n)\n",
    "n = 512\n",
    "padded_ids = []\n",
    "for each in ids:\n",
    "    padded = np.array(each[:n] + [0]*(n - len(each)))\n",
    "    padded_ids.append(padded)\n",
    "\n",
    "# creating the attention mask to distinguish tokens we are interested in\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded_ids != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2472, 5203, 2909, 2243, 2320, 2056, 1036, 2045, 1005, 1055, 1037, 2200, 2460, 3292, 2090, 2152, 2396, 1998, 11669, 1010, 1998, 11669, 2008, 3397, 13675, 16103, 2791, 2003, 2011, 2023, 2200, 3737, 20388, 2000, 2396, 1005, 1012, 2023, 4861, 11859, 2010, 5988, 6669, 1010, 1037, 2200, 4310, 2303, 1997, 2147, 2008, 2950, 4438, 2754, 17241, 1010, 6172, 1998, 2162, 3152, 1010, 2530, 2015, 1998, 1997, 2607, 1010, 2010, 3297, 11463, 7716, 14672, 2015, 1012, 2909, 2243, 1005, 1055, 11463, 7716, 14672, 2015, 2020, 1010, 2004, 1996, 2200, 2773, 27353, 1010, 16547, 2007, 2189, 1012, 1996, 2189, 4520, 1996, 4309, 2005, 2010, 3040, 3993, 2806, 1010, 1998, 2296, 6909, 1997, 2010, 8248, 1006, 2909, 2243, 2001, 2036, 1037, 5276, 1007, 3727, 1037, 3928, 3746, 2006, 1996, 3898, 1011, 2357, 1011, 10683, 1012, 2021, 2023, 7110, 1005, 1056, 2166, 2021, 2049, 6630, 1010, 2019, 20017, 1997, 2166, 1012, 2909, 2243, 2196, 2699, 2000, 2265, 4507, 1010, 2006, 1996, 10043, 1012, 3904, 1997, 1996, 5501, 1997, 2010, 4245, 2081, 1037, 2488, 2224, 1997, 2035, 1996, 4087, 5733, 3024, 2011, 5365, 1006, 2087, 5546, 6627, 8713, 12898, 2099, 1007, 2000, 10782, 1996, 7976, 2013, 1996, 2613, 2518, 1012, 2292, 1005, 1055, 3342, 2008, 2010, 3585, 2558, 19680, 2015, 2007, 1996, 2051, 2043, 5365, 3152, 2357, 2049, 3086, 2046, 1996, 2591, 3689, 1006, 2304, 6277, 8894, 1010, 8443, 2302, 1037, 3426, 1007, 1012, 2909, 2243, 2467, 2354, 2008, 5988, 2001, 3214, 2000, 2022, 2242, 2842, 1012, 2178, 1997, 2909, 2243, 1005, 1055, 8635, 7680, 7849, 10057, 2023, 1024, 1036, 2017, 2064, 1005, 1056, 3362, 1010, 2030, 3543, 1010, 1996, 2613, 1012, 2017, 2074, 2156, 16055, 1012, 2065, 2017, 3046, 2000, 10616, 8404, 2993, 2115, 3093, 2069, 3113, 3221, 1005, 1012, 1045, 13366, 2100, 10334, 2008, 2038, 2464, 2517, 2006, 1996, 3612, 2000, 4175, 1996, 3815, 1997, 13536, 1998, 4871, 7686, 2008, 3711, 2006, 3898, 1012, 2028, 4515, 2039, 3228, 2039, 1012, 3568, 1010, 2057, 2024, 1999, 1037, 2534, 2440, 1997, 13536, 2073, 2045, 1005, 1055, 2053, 4489, 2090, 2613, 1998, 2049, 6270, 6100, 1012, 6343, 2064, 2360, 2008, 1996, 21681, 2024, 2613, 2111, 1012, 2008, 2237, 7110, 1005, 1056, 2613, 2593, 1010, 2007, 2216, 22293, 3514, 15856, 2035, 2058, 1996, 2173, 1012, 2061, 1999, 2023, 8391, 1996, 3772, 2003, 5360, 1010, 1996, 25545, 2063, 2003, 8275, 1010, 1996, 7577, 2003, 5710, 1012, 2673, 2003, 3724, 1037, 2210, 2978, 2125, 1996, 5787, 1006, 1996, 4424, 9530, 17048, 10708, 1997, 9984, 16321, 2007, 1996, 3514, 3578, 1010, 2005, 2742, 1007, 1012, 2909, 2243, 2001, 21289, 1998, 14833, 21885, 2075, 2012, 1996, 2168, 2051, 1012, 1036, 1996, 12113, 2024, 1996, 2472, 1005, 1055, 4301, 1025, 1996, 7497, 2003, 2010, 4695, 1005, 1012, 1999, 2517, 2006, 1996, 3612, 2057, 3582, 1996, 2991, 1997, 1037, 3151, 2126, 1997, 2166, 2119, 1999, 1037, 14965, 2389, 2126, 1998, 1999, 3408, 1997, 2422, 1998, 6281, 1012, 1996, 21681, 2015, 2160, 1010, 2007, 2049, 2367, 3798, 4198, 2011, 1996, 12313, 10714, 5748, 1999, 1037, 9975, 19240, 7476, 2126, 1012, 1037, 2160, 2008, 12950, 1037, 19049, 1010, 2008, 2053, 2283, 2064, 15138, 2039, 1012, 2004, 102]\n"
     ]
    }
   ],
   "source": [
    "print(ids[2026])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of vector IDs (padded) and the list of attention masks\n",
    "ids_list = []\n",
    "attention_mask_list = []\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "for input_text in corpus[:200]:\n",
    "    ids = tokenizer.encode(input_text.lower(), add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "    padded = np.array(ids + [0]*(max_length - len(ids)))\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    ids_list.append(padded)\n",
    "    attention_mask_list.append(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tqdm lilbrary displays progress of  operation\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for i in tqdm(range(int(8e6))):\n",
    "    pass\n",
    "\n",
    "# the progress bar will appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The BERT model creates embeddings in batches\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a loop for the batches\n",
    "# creating an empty list of review embeddings \n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(len(ids_list) // batch_size)):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the data into a tensor format. \n",
    "# putting together vectors of ids (of tokens) to a tensor\n",
    "ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)])\n",
    "# putting together vectors of attention masks to a tensor\n",
    "attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_size*(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass the data and the mask to the model to obtain embeddings for the batch:\n",
    "batch_embeddings = model(ids_batch, attention_mask=attention_mask_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch_embeddings = model(ids_batch, attention_mask=attention_mask_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the required elements from the tensor and add the list of all the embeddings:\n",
    "# converting elements of tensor to numpy.array with the numpy() function\n",
    "embeddings.append(batch_embeddings[0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting all the above together, we get this loop:\n",
    "batch_size = 100\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(len(ids_list) // batch_size)):\n",
    "    \n",
    "    ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)])\n",
    "    attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_size*(i+1)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(ids_batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "    embeddings.append(batch_embeddings[0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all the embeddings in a matrix of features:\n",
    "features = np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
