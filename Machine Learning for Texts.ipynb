{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer  = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heaven'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('heavens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"All models are wrong, but some are useful.\"\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'model', 'are', 'wrong', ',', 'but', 'some', 'are', 'useful', '.']\n"
     ]
    }
   ],
   "source": [
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all model are wrong , but some are useful .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-67741b070562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parser'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ner'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "doc = nlp(text.lower())\n",
    "\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization using spaCy\n",
    "import pandas as pd\n",
    "import random             # in order to select a random review\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small.tsv', sep='\\t')\n",
    "corpus = data['review']\n",
    "\n",
    "def lemmatize(text):\n",
    "\n",
    "    # < write code here >\n",
    "    text = nlp(text.lower())\n",
    "    lemmas = [token.lemma_ for token in text]\n",
    "    text_lemmas = \" \".join(lemmas)\n",
    "    return text_lemmas\n",
    "\n",
    "# store the review index in the review_idx variable\n",
    "# either as a random number or a fixed value, e.g. 2557 \n",
    "#review_idx = random.randint(0, len(corpus)-1)\n",
    "review_idx = 2557\n",
    "\n",
    "review = corpus[review_idx]\n",
    "\n",
    "print(\"The original text:\", review)\n",
    "print()\n",
    "print(\"The lemmatized text:\", lemmatize(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pattern' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-978b5ccfa074>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# substitution — what each pattern match should be substituted with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# text — the text which the function scans for pattern matches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pattern' is not defined"
     ]
    }
   ],
   "source": [
    "# pattern\n",
    "# substitution — what each pattern match should be substituted with\n",
    "# text — the text which the function scans for pattern matches\n",
    "re.sub(pattern, substitution, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "\n",
      "Hello!\\n\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello!\\n\")\n",
    "\n",
    "print(r\"Hello!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a range of letters is indicated by a hyphen:\n",
    "# a-z = abcdefghijklmnopqrstuvwxyz\n",
    "r\"[a-zA-Z]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find apostrophes as well\n",
    "r\"[a-zA-Z']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review text\n",
    "text = \"\"\"\n",
    "I liked this show from the first episode I saw, which was the \"Rhapsody in Blue\" episode (for those that don't know what that is, the Zan going insane and becoming pau lvl 10 ep). Best visuals and special effects I've seen on a television series, nothing like it anywhere.\n",
    "\"\"\"\n",
    "text_sub = re.sub(r\"[^a-zA-Z']\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = text_sub.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I liked this show from the first episode I saw which was the Rhapsody in Blue episode for those that don't know what that is the Zan going insane and becoming pau lvl ep Best visuals and special effects I've seen on a television series nothing like it anywhere\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'liked', 'this', 'show']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"            I   liked   this   show   \"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I liked this show'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(['I', 'liked', 'this', 'show'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random             # in order to select a random review\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small.tsv', sep='\\t')\n",
    "corpus = data['review']\n",
    "\n",
    "def clear_text(text):\n",
    "    \n",
    "    # < write code here >\n",
    "    text = re.sub(r\"[^a-zA-Z']\", \" \", text)\n",
    "    text = text.split()\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        lemmas.append(token.lemma_)\n",
    "        \n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# store the review index in the review_idx variable\n",
    "# either as a random number or a fixed value, e.g. 2557 \n",
    "review_idx = random.randint(0, len(corpus)-1)\n",
    "# review_idx = 2557\n",
    "\n",
    "review = corpus[review_idx]\n",
    "\n",
    "print(\"The original text:\", review)\n",
    "print()\n",
    "print(\"The lemmatized text:\", lemmatize(clear_text(review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('datasets/imdb_reviews_small_lemm_train.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>original_title</th>\n",
       "      <th>review</th>\n",
       "      <th>review_lemm</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>tt0277615</td>\n",
       "      <td>Bug</td>\n",
       "      <td>As a producer of indie movies and a harsh crit...</td>\n",
       "      <td>as a producer of indie movie and a harsh criti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>tt0298482</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>Usually, I know after the first minute of a mo...</td>\n",
       "      <td>usually i know after the first minute of a mov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>tt0062751</td>\n",
       "      <td>Boom</td>\n",
       "      <td>\"Boom\" has garnered itself a something of a re...</td>\n",
       "      <td>boom have garner -PRON- a something of a reput...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>tt0114287</td>\n",
       "      <td>Rob Roy</td>\n",
       "      <td>This is the moving tale of Scotland's legendar...</td>\n",
       "      <td>this be the move tale of scotland 's legendary...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>tt0116277</td>\n",
       "      <td>The Fan</td>\n",
       "      <td>What a bad movie. I'm really surprised that De...</td>\n",
       "      <td>what a bad movie -PRON- be really surprised th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>tt0049966</td>\n",
       "      <td>Written on the Wind</td>\n",
       "      <td>On the surface, \"Written on the Wind\" is a lur...</td>\n",
       "      <td>on the surface write on the wind be a lurid gl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>tt0115907</td>\n",
       "      <td>City Hall</td>\n",
       "      <td>I read the negative comments before viewing th...</td>\n",
       "      <td>i read the negative comment before view this f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>tt0139388</td>\n",
       "      <td>It Had to Be You</td>\n",
       "      <td>I'd have given this film a few stars, simply b...</td>\n",
       "      <td>-PRON- 'd have give this film a few star simpl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>tt0114287</td>\n",
       "      <td>Rob Roy</td>\n",
       "      <td>From the excellent acting of an extremely impr...</td>\n",
       "      <td>from the excellent acting of an extremely impr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>tt0493424</td>\n",
       "      <td>Attack Force</td>\n",
       "      <td>Steven Seagal....how could you be a part of su...</td>\n",
       "      <td>steven seagal how could -PRON- be a part of su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tconst       original_title  \\\n",
       "302   tt0277615                  Bug   \n",
       "1127  tt0298482              Nothing   \n",
       "265   tt0062751                 Boom   \n",
       "1363  tt0114287              Rob Roy   \n",
       "1695  tt0116277              The Fan   \n",
       "2021  tt0049966  Written on the Wind   \n",
       "409   tt0115907            City Hall   \n",
       "863   tt0139388     It Had to Be You   \n",
       "1377  tt0114287              Rob Roy   \n",
       "161   tt0493424         Attack Force   \n",
       "\n",
       "                                                 review  \\\n",
       "302   As a producer of indie movies and a harsh crit...   \n",
       "1127  Usually, I know after the first minute of a mo...   \n",
       "265   \"Boom\" has garnered itself a something of a re...   \n",
       "1363  This is the moving tale of Scotland's legendar...   \n",
       "1695  What a bad movie. I'm really surprised that De...   \n",
       "2021  On the surface, \"Written on the Wind\" is a lur...   \n",
       "409   I read the negative comments before viewing th...   \n",
       "863   I'd have given this film a few stars, simply b...   \n",
       "1377  From the excellent acting of an extremely impr...   \n",
       "161   Steven Seagal....how could you be a part of su...   \n",
       "\n",
       "                                            review_lemm  pos  \n",
       "302   as a producer of indie movie and a harsh criti...    1  \n",
       "1127  usually i know after the first minute of a mov...    0  \n",
       "265   boom have garner -PRON- a something of a reput...    0  \n",
       "1363  this be the move tale of scotland 's legendary...    1  \n",
       "1695  what a bad movie -PRON- be really surprised th...    0  \n",
       "2021  on the surface write on the wind be a lurid gl...    1  \n",
       "409   i read the negative comment before view this f...    1  \n",
       "863   -PRON- 'd have give this film a few star simpl...    1  \n",
       "1377  from the excellent acting of an extremely impr...    1  \n",
       "161   steven seagal how could -PRON- be a part of su...    0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data['review_norm'] = data['review'].apply(lambda x: re.sub(r\"[^a-zA-Z]\", \" \", x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>original_title</th>\n",
       "      <th>review</th>\n",
       "      <th>review_lemm</th>\n",
       "      <th>pos</th>\n",
       "      <th>review_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>tt0164052</td>\n",
       "      <td>Hollow Man</td>\n",
       "      <td>A Pentagon science team seem to have perfected...</td>\n",
       "      <td>a pentagon science team seem to have perfect a...</td>\n",
       "      <td>1</td>\n",
       "      <td>a pentagon science team seem to have perfected...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>tt0101811</td>\n",
       "      <td>Enchanted April</td>\n",
       "      <td>For anyone who's judged others at first meetin...</td>\n",
       "      <td>for anyone who be judge other at first meeting...</td>\n",
       "      <td>1</td>\n",
       "      <td>for anyone who s judged others at first meetin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>tt0331175</td>\n",
       "      <td>Any Way the Wind Blows</td>\n",
       "      <td>This whirling movie looks more like a combinat...</td>\n",
       "      <td>this whirl movie look more like a combination ...</td>\n",
       "      <td>0</td>\n",
       "      <td>this whirling movie looks more like a combinat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>tt0076085</td>\n",
       "      <td>Una giornata particolare</td>\n",
       "      <td>Una giornata particolare is a film which has m...</td>\n",
       "      <td>una giornata particolare be a film which have ...</td>\n",
       "      <td>1</td>\n",
       "      <td>una giornata particolare is a film which has m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>tt0164052</td>\n",
       "      <td>Hollow Man</td>\n",
       "      <td>Paul Verhoeven finally bombed out on this one...</td>\n",
       "      <td>paul verhoeven finally bomb out on this one -P...</td>\n",
       "      <td>0</td>\n",
       "      <td>paul verhoeven finally bombed out on this one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>tt0076085</td>\n",
       "      <td>Una giornata particolare</td>\n",
       "      <td>Ettore Scola is one of the most important Ital...</td>\n",
       "      <td>ettore scola be one of the most important ital...</td>\n",
       "      <td>1</td>\n",
       "      <td>ettore scola is one of the most important ital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>tt0231775</td>\n",
       "      <td>Down to Earth</td>\n",
       "      <td>The main character Lance Barton gets killed an...</td>\n",
       "      <td>the main character lance barton get kill and t...</td>\n",
       "      <td>0</td>\n",
       "      <td>the main character lance barton gets killed an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>tt0074811</td>\n",
       "      <td>Le locataire</td>\n",
       "      <td>Roman Polanski masterfully directs this sort o...</td>\n",
       "      <td>roman polanski masterfully direct this sort of...</td>\n",
       "      <td>1</td>\n",
       "      <td>roman polanski masterfully directs this sort o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>tt0139388</td>\n",
       "      <td>It Had to Be You</td>\n",
       "      <td>Sure, this movie is sappy and sweet and full o...</td>\n",
       "      <td>sure this movie be sappy and sweet and full of...</td>\n",
       "      <td>1</td>\n",
       "      <td>sure  this movie is sappy and sweet and full o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>tt0190524</td>\n",
       "      <td>Left Behind</td>\n",
       "      <td>Surprisingly enough does movie does have some ...</td>\n",
       "      <td>surprisingly enough do movie do have some rede...</td>\n",
       "      <td>0</td>\n",
       "      <td>surprisingly enough does movie does have some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>tt0298482</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>Well, \"Cube\" (1997), Vincenzo's first movie, w...</td>\n",
       "      <td>well cube vincenzo 's first movie be one of th...</td>\n",
       "      <td>1</td>\n",
       "      <td>well   cube          vincenzo s first movie  w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>tt0343135</td>\n",
       "      <td>Along Came Polly</td>\n",
       "      <td>Make no bones about it. There are a lot of thi...</td>\n",
       "      <td>make no bone about -PRON- there be a lot of th...</td>\n",
       "      <td>0</td>\n",
       "      <td>make no bones about it  there are a lot of thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>tt0443632</td>\n",
       "      <td>The Sentinel</td>\n",
       "      <td>Seeing as Keifer Sutherland plays my favorite ...</td>\n",
       "      <td>see as keifer sutherland play -PRON- favorite ...</td>\n",
       "      <td>1</td>\n",
       "      <td>seeing as keifer sutherland plays my favorite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>tt0076085</td>\n",
       "      <td>Una giornata particolare</td>\n",
       "      <td>First of all for this movie I just have one wo...</td>\n",
       "      <td>first of all for this movie i just have one wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>first of all for this movie i just have one wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>tt0298482</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>Director Vincenzo Natali first showed his penc...</td>\n",
       "      <td>director vincenzo natali first show -PRON- pen...</td>\n",
       "      <td>1</td>\n",
       "      <td>director vincenzo natali first showed his penc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>tt0190524</td>\n",
       "      <td>Left Behind</td>\n",
       "      <td>I'm not a fan of the Left Behind book series -...</td>\n",
       "      <td>-PRON- be not a fan of the leave behind book s...</td>\n",
       "      <td>0</td>\n",
       "      <td>i m not a fan of the left behind book series  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>tt0076085</td>\n",
       "      <td>Una giornata particolare</td>\n",
       "      <td>A SPECIAL DAY (Ettore Scola - Italy/Canada 197...</td>\n",
       "      <td>a special day ettore scola italy canada every ...</td>\n",
       "      <td>1</td>\n",
       "      <td>a special day  ettore scola   italy canada    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>tt0031038</td>\n",
       "      <td>The Amazing Mr. Williams</td>\n",
       "      <td>What was there about 1939 that helped produce ...</td>\n",
       "      <td>what be there about that help produce so many ...</td>\n",
       "      <td>1</td>\n",
       "      <td>what was there about      that helped produce ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>tt0079103</td>\n",
       "      <td>Elvis</td>\n",
       "      <td>Kurt Russell, whose career started when he kic...</td>\n",
       "      <td>kurt russell whose career start when -PRON- ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>kurt russell  whose career started when he kic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>tt0844330</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>I love Jane Austen's stories. I've only read t...</td>\n",
       "      <td>i love jane austen 's story -PRON- have only r...</td>\n",
       "      <td>0</td>\n",
       "      <td>i love jane austen s stories  i ve only read t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tconst            original_title  \\\n",
       "797   tt0164052                Hollow Man   \n",
       "578   tt0101811           Enchanted April   \n",
       "104   tt0331175    Any Way the Wind Blows   \n",
       "84    tt0076085  Una giornata particolare   \n",
       "778   tt0164052                Hollow Man   \n",
       "83    tt0076085  Una giornata particolare   \n",
       "493   tt0231775             Down to Earth   \n",
       "1844  tt0074811              Le locataire   \n",
       "858   tt0139388          It Had to Be You   \n",
       "900   tt0190524               Left Behind   \n",
       "1138  tt0298482                   Nothing   \n",
       "94    tt0343135          Along Came Polly   \n",
       "1822  tt0443632              The Sentinel   \n",
       "78    tt0076085  Una giornata particolare   \n",
       "1132  tt0298482                   Nothing   \n",
       "878   tt0190524               Left Behind   \n",
       "79    tt0076085  Una giornata particolare   \n",
       "1619  tt0031038  The Amazing Mr. Williams   \n",
       "528   tt0079103                     Elvis   \n",
       "1276  tt0844330                Persuasion   \n",
       "\n",
       "                                                 review  \\\n",
       "797   A Pentagon science team seem to have perfected...   \n",
       "578   For anyone who's judged others at first meetin...   \n",
       "104   This whirling movie looks more like a combinat...   \n",
       "84    Una giornata particolare is a film which has m...   \n",
       "778    Paul Verhoeven finally bombed out on this one...   \n",
       "83    Ettore Scola is one of the most important Ital...   \n",
       "493   The main character Lance Barton gets killed an...   \n",
       "1844  Roman Polanski masterfully directs this sort o...   \n",
       "858   Sure, this movie is sappy and sweet and full o...   \n",
       "900   Surprisingly enough does movie does have some ...   \n",
       "1138  Well, \"Cube\" (1997), Vincenzo's first movie, w...   \n",
       "94    Make no bones about it. There are a lot of thi...   \n",
       "1822  Seeing as Keifer Sutherland plays my favorite ...   \n",
       "78    First of all for this movie I just have one wo...   \n",
       "1132  Director Vincenzo Natali first showed his penc...   \n",
       "878   I'm not a fan of the Left Behind book series -...   \n",
       "79    A SPECIAL DAY (Ettore Scola - Italy/Canada 197...   \n",
       "1619  What was there about 1939 that helped produce ...   \n",
       "528   Kurt Russell, whose career started when he kic...   \n",
       "1276  I love Jane Austen's stories. I've only read t...   \n",
       "\n",
       "                                            review_lemm  pos  \\\n",
       "797   a pentagon science team seem to have perfect a...    1   \n",
       "578   for anyone who be judge other at first meeting...    1   \n",
       "104   this whirl movie look more like a combination ...    0   \n",
       "84    una giornata particolare be a film which have ...    1   \n",
       "778   paul verhoeven finally bomb out on this one -P...    0   \n",
       "83    ettore scola be one of the most important ital...    1   \n",
       "493   the main character lance barton get kill and t...    0   \n",
       "1844  roman polanski masterfully direct this sort of...    1   \n",
       "858   sure this movie be sappy and sweet and full of...    1   \n",
       "900   surprisingly enough do movie do have some rede...    0   \n",
       "1138  well cube vincenzo 's first movie be one of th...    1   \n",
       "94    make no bone about -PRON- there be a lot of th...    0   \n",
       "1822  see as keifer sutherland play -PRON- favorite ...    1   \n",
       "78    first of all for this movie i just have one wo...    1   \n",
       "1132  director vincenzo natali first show -PRON- pen...    1   \n",
       "878   -PRON- be not a fan of the leave behind book s...    0   \n",
       "79    a special day ettore scola italy canada every ...    1   \n",
       "1619  what be there about that help produce so many ...    1   \n",
       "528   kurt russell whose career start when -PRON- ki...    1   \n",
       "1276  i love jane austen 's story -PRON- have only r...    0   \n",
       "\n",
       "                                            review_norm  \n",
       "797   a pentagon science team seem to have perfected...  \n",
       "578   for anyone who s judged others at first meetin...  \n",
       "104   this whirling movie looks more like a combinat...  \n",
       "84    una giornata particolare is a film which has m...  \n",
       "778    paul verhoeven finally bombed out on this one...  \n",
       "83    ettore scola is one of the most important ital...  \n",
       "493   the main character lance barton gets killed an...  \n",
       "1844  roman polanski masterfully directs this sort o...  \n",
       "858   sure  this movie is sappy and sweet and full o...  \n",
       "900   surprisingly enough does movie does have some ...  \n",
       "1138  well   cube          vincenzo s first movie  w...  \n",
       "94    make no bones about it  there are a lot of thi...  \n",
       "1822  seeing as keifer sutherland plays my favorite ...  \n",
       "78    first of all for this movie i just have one wo...  \n",
       "1132  director vincenzo natali first showed his penc...  \n",
       "878   i m not a fan of the left behind book series  ...  \n",
       "79    a special day  ettore scola   italy canada    ...  \n",
       "1619  what was there about      that helped produce ...  \n",
       "528   kurt russell  whose career started when he kic...  \n",
       "1276  i love jane austen s stories  i ve only read t...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "text = \"\"\"For want of a nail the shoe was lost. For want of a shoe the horse was lost. For want of a horse the rider was lost.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.lemma_ for token in doc if not token.is_punct]\n",
    "\n",
    "bow = Counter(tokens)\n",
    "\n",
    "vector = [bow[token] for token in sorted(bow)]\n",
    "\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "    Natural-language 56 processing (NLP) is an area of\n",
    "    computer 34science and 66artificial intelligence\n",
    "    12concerned with the 89interactions between computers\n",
    "    and human (natural)33 languages.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n natural language processing nlp',\n",
       " 'natural language processing nlp is',\n",
       " 'language processing nlp is an',\n",
       " 'processing nlp is an area',\n",
       " 'nlp is an area of\\n',\n",
       " 'is an area of\\n computer',\n",
       " 'an area of\\n computer science',\n",
       " 'area of\\n computer science and',\n",
       " 'of\\n computer science and artificial',\n",
       " 'computer science and artificial intelligence\\n',\n",
       " 'science and artificial intelligence\\n concerned',\n",
       " 'and artificial intelligence\\n concerned with',\n",
       " 'artificial intelligence\\n concerned with the',\n",
       " 'intelligence\\n concerned with the interactions',\n",
       " 'concerned with the interactions between',\n",
       " 'with the interactions between computers\\n',\n",
       " 'the interactions between computers\\n and',\n",
       " 'interactions between computers\\n and human',\n",
       " 'between computers\\n and human natural',\n",
       " 'computers\\n and human natural languages',\n",
       " 'and human natural languages \\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ngrams(s, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', 'five']\n",
      "[['one', 'two', 'three', 'four', 'five'], ['two', 'three', 'four', 'five'], ['three', 'four', 'five']]\n",
      "[('one', 'two', 'three'), ('two', 'three', 'four'), ('three', 'four', 'five')]\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "s = \"one two three four five\"\n",
    "\n",
    "tokens = s.split(\" \")\n",
    "# tokens = [\"one\", \"two\", \"three\", \"four\", \"five\"]\n",
    "print(tokens)\n",
    "\n",
    "sequences = [tokens[i:] for i in range(3)]\n",
    "# The above will generate sequences of tokens starting\n",
    "# from different elements of the list of tokens.\n",
    "# The parameter in the range() function controls\n",
    "# how many sequences to generate.\n",
    "#\n",
    "# sequences = [\n",
    "#   ['one', 'two', 'three', 'four', 'five'],\n",
    "#   ['two', 'three', 'four', 'five'],\n",
    "#   ['three', 'four', 'five']]\n",
    "print(sequences)\n",
    "\n",
    "bigrams = zip(*sequences)\n",
    "print(list(bigrams))\n",
    "# The zip function takes the sequences as a list of inputs\n",
    "# (using the * operator, this is equivalent to\n",
    "# zip(sequences[0], sequences[1], sequences[2]).\n",
    "# Each tuple it returns will contain one element from\n",
    "# each of the sequences.\n",
    "# \n",
    "# To inspect the content of bigrams, try:\n",
    "# print(list(bigrams))\n",
    "# which will give the following:\n",
    "#\n",
    "# [\n",
    "#   ('one', 'two', 'three'),\n",
    "#   ('two', 'three', 'four'),\n",
    "#   ('three', 'four', 'five')\n",
    "# ]\n",
    "#\n",
    "# Note: even though the first sequence has 5 elements,\n",
    "# zip will stop after returning 3 tuples, because the\n",
    "# last sequence only has 3 elements. In other words,\n",
    "# the zip function automatically handles the ending of\n",
    "# the n-gram generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "s = s.lower()\n",
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "output = list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 'two', 'three'), ('two', 'three', 'four'), ('three', 'four', 'five')]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'for want of a nail the shoe be lose',\n",
    "    'for want of a shoe the horse be lose',\n",
    "    'for want of a horse the rider be lose',\n",
    "    'for want of a rider the message be lose',\n",
    "    'for want of a message the battle be lose',\n",
    "    'for want of a battle the kingdom be lose',\n",
    "    'and all for the want of a horseshoe nail'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The counter extracts unique words from the corpus and counts how many times they appear in each text of the corpus. \n",
    "#The counter doesn't count separate letters.\n",
    "# bow = bag of words\n",
    "bow = count_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 16)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape\n",
    "#16 unique words, 7 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1]\n",
      " [0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1]\n",
      " [0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1]\n",
      " [0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1]\n",
      " [1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'and',\n",
       " 'battle',\n",
       " 'be',\n",
       " 'for',\n",
       " 'horse',\n",
       " 'horseshoe',\n",
       " 'kingdom',\n",
       " 'lose',\n",
       " 'message',\n",
       " 'nail',\n",
       " 'of',\n",
       " 'rider',\n",
       " 'shoe',\n",
       " 'the',\n",
       " 'want']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The list of unique words in the bag is called a vocabulary. \n",
    "#It's stored in the counter and can be accessed by calling the get_feature_names() method:\n",
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams\n",
    "count_vect = CountVectorizer(ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2 = count_vect.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 1 0 1 1]\n",
      " [0 1 0 0 1 0 0 0 1 1]\n",
      " [0 1 0 0 1 0 0 1 0 1]\n",
      " [0 0 0 0 1 1 0 1 0 1]\n",
      " [1 0 0 0 1 1 0 0 0 1]\n",
      " [1 0 0 1 1 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['battle',\n",
       " 'horse',\n",
       " 'horseshoe',\n",
       " 'kingdom',\n",
       " 'lose',\n",
       " 'message',\n",
       " 'nail',\n",
       " 'rider',\n",
       " 'shoe',\n",
       " 'want']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# < write code here >\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small_lemm.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "# create a bag-of-words without checking for stop words\n",
    "# < write code here >\n",
    "count_vect = CountVectorizer()\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "print(\"The BoW size with stop words:\", bow.shape)\n",
    "\n",
    "# create a bag-of-words with checking for stop words\n",
    "# < write code here >\n",
    "stop_words = set(stopwords.words('english'))\n",
    "count_vect = CountVectorizer(stop_words=stop_words)\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(\"The BoW size without stop words:\", bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small_lemm.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "# create an n-gram with n=2 and store it in the n_gram variable\n",
    "\n",
    "# < write code here >\n",
    "count_vect = CountVectorizer(ngram_range=(2, 2))\n",
    "n_gram = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(\"The size of 2-gram:\", n_gram.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = count_tf_idf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TF-IDF matrix size: (2027, 18036)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "# import TfidfVectorizer\n",
    "# < write code here >\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = pd.read_csv('datasets/imdb_reviews_small_lemm_train.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "stop_words = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "tf_idf = count_tf_idf.fit_transform(corpus)\n",
    "# < write code here >\n",
    "\n",
    "print(\"The TF-IDF matrix size:\", tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2027,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_valid = accuracy_score(pred, target_valid)\n",
    "print(accuracy_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       i see this movie last year in medium class and...\n",
       "1       i must admit there be few book with correspond...\n",
       "2       i think that the shot and light be very poor w...\n",
       "3       a few week ago i read the classic george orwel...\n",
       "4       i see this movie literally directly after fini...\n",
       "                              ...                        \n",
       "2022    director douglas sirk score again with this th...\n",
       "2023    spoiler spoiler release in and consider quite ...\n",
       "2024    fabulous film rent the dvd recently and be flo...\n",
       "2025    rich alcoholic robert stack fall in love with ...\n",
       "2026    director douglas sirk once say ` there be a ve...\n",
       "Name: review_lemm, Length: 2027, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "# import TfidfVectorizer\n",
    "# < write code here >\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = pd.read_csv('datasets/imdb_reviews_small_lemm_train.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TF-IDF matrix size: (2027, 18036)\n"
     ]
    }
   ],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "tf_idf = count_tf_idf.fit_transform(corpus)\n",
    "\n",
    "print(\"The TF-IDF matrix size:\", tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2027x18036 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 196664 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tconst', 'original_title', 'review', 'review_lemm', 'pos'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "2022    1\n",
       "2023    1\n",
       "2024    1\n",
       "2025    1\n",
       "2026    1\n",
       "Name: pos, Length: 2027, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = data['pos']\n",
    "#features = corpus\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    tf_idf, target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1621x18036 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 159016 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1254    0\n",
       "898     0\n",
       "1187    0\n",
       "37      1\n",
       "1200    0\n",
       "       ..\n",
       "1359    1\n",
       "516     0\n",
       "122     0\n",
       "641     1\n",
       "457     1\n",
       "Name: pos, Length: 406, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0, solver='liblinear')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, solver='liblinear')\n",
    "model.fit(tf_idf, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('datasets/imdb_reviews_small_lemm_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = test_set['review_lemm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TF-IDF matrix size: (2220, 18036)\n"
     ]
    }
   ],
   "source": [
    "test_tf_idf = count_tf_idf.transform(test_target)\n",
    "\n",
    "print(\"The TF-IDF matrix size:\", test_tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2220,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['pos'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>original_title</th>\n",
       "      <th>review</th>\n",
       "      <th>review_lemm</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0108999</td>\n",
       "      <td>...And the Earth Did Not Swallow Him</td>\n",
       "      <td>I rented this movie from a local library witho...</td>\n",
       "      <td>i rent this movie from a local library without...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0108999</td>\n",
       "      <td>...And the Earth Did Not Swallow Him</td>\n",
       "      <td>The movie \". . . And The Earth Did not Swallow...</td>\n",
       "      <td>the movie and the earth do not swallow -PRON- ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0108999</td>\n",
       "      <td>...And the Earth Did Not Swallow Him</td>\n",
       "      <td>I was very moved by the young life experiences...</td>\n",
       "      <td>i be very move by the young life experience of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0108999</td>\n",
       "      <td>...And the Earth Did Not Swallow Him</td>\n",
       "      <td>Recently finally available in DVD (11/11/08), ...</td>\n",
       "      <td>recently finally available in dvd severo p rez...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0063308</td>\n",
       "      <td>Un minuto per pregare, un istante per morire</td>\n",
       "      <td>I saw this movie over 20 years ago and had rat...</td>\n",
       "      <td>i see this movie over year ago and have rather...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>tt0472278</td>\n",
       "      <td>Vampire Assassin</td>\n",
       "      <td>Ron Hall pulls a triple threat as he writes, d...</td>\n",
       "      <td>ron hall pull a triple threat as -PRON- write ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>tt0832971</td>\n",
       "      <td>Vanaja</td>\n",
       "      <td>Winning 26 out of the 28 awards it was nominat...</td>\n",
       "      <td>win out of the award -PRON- be nominate for th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>tt0832971</td>\n",
       "      <td>Vanaja</td>\n",
       "      <td>Vanaja is a film of superlatives. It has an ex...</td>\n",
       "      <td>vanaja be a film of superlative -PRON- have an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>tt0832971</td>\n",
       "      <td>Vanaja</td>\n",
       "      <td>This is not your typical Indian film. There is...</td>\n",
       "      <td>this be not -PRON- typical indian film there b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>tt0832971</td>\n",
       "      <td>Vanaja</td>\n",
       "      <td>Vanaja (2006), written and directed by Rajnesh...</td>\n",
       "      <td>vanaja write and direct by rajnesh domalpalli ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2220 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tconst                                original_title  \\\n",
       "0     tt0108999          ...And the Earth Did Not Swallow Him   \n",
       "1     tt0108999          ...And the Earth Did Not Swallow Him   \n",
       "2     tt0108999          ...And the Earth Did Not Swallow Him   \n",
       "3     tt0108999          ...And the Earth Did Not Swallow Him   \n",
       "4     tt0063308  Un minuto per pregare, un istante per morire   \n",
       "...         ...                                           ...   \n",
       "2215  tt0472278                              Vampire Assassin   \n",
       "2216  tt0832971                                        Vanaja   \n",
       "2217  tt0832971                                        Vanaja   \n",
       "2218  tt0832971                                        Vanaja   \n",
       "2219  tt0832971                                        Vanaja   \n",
       "\n",
       "                                                 review  \\\n",
       "0     I rented this movie from a local library witho...   \n",
       "1     The movie \". . . And The Earth Did not Swallow...   \n",
       "2     I was very moved by the young life experiences...   \n",
       "3     Recently finally available in DVD (11/11/08), ...   \n",
       "4     I saw this movie over 20 years ago and had rat...   \n",
       "...                                                 ...   \n",
       "2215  Ron Hall pulls a triple threat as he writes, d...   \n",
       "2216  Winning 26 out of the 28 awards it was nominat...   \n",
       "2217  Vanaja is a film of superlatives. It has an ex...   \n",
       "2218  This is not your typical Indian film. There is...   \n",
       "2219  Vanaja (2006), written and directed by Rajnesh...   \n",
       "\n",
       "                                            review_lemm  pos  \n",
       "0     i rent this movie from a local library without...    0  \n",
       "1     the movie and the earth do not swallow -PRON- ...    1  \n",
       "2     i be very move by the young life experience of...    1  \n",
       "3     recently finally available in dvd severo p rez...    1  \n",
       "4     i see this movie over year ago and have rather...    0  \n",
       "...                                                 ...  ...  \n",
       "2215  ron hall pull a triple threat as -PRON- write ...    0  \n",
       "2216  win out of the award -PRON- be nominate for th...    1  \n",
       "2217  vanaja be a film of superlative -PRON- have an...    1  \n",
       "2218  this be not -PRON- typical indian film there b...    1  \n",
       "2219  vanaja write and direct by rajnesh domalpalli ...    1  \n",
       "\n",
       "[2220 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_csv('predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9224f0faab3d4466bb5f60046eaed700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize the tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2009, 2003, 2200, 18801, 2000, 2224, 19081, 102]\n"
     ]
    }
   ],
   "source": [
    "#Convert the text into IDs of tokens, and the BERT tokenizer will return IDs of tokens rather than tokens\n",
    "example = 'It is very handy to use transformers'\n",
    "ids = tokenizer.encode(example, add_special_tokens=True)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  2009  2003  2200 18801  2000  2224 19081   102     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "#BERT accepts vectors of a fixed length, e.g. of 512 tokens. \n",
    "n = 512\n",
    "\n",
    "padded = np.array(ids[:n] + [0]*(n - len(ids)))\n",
    "\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "#create a mask for the important tokens, indicating zero and non-zero values\n",
    "#zeros do not carry significant information\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "data = pd.read_csv('datasets/imdb_reviews_small_lemm_train.tsv', sep='\\t')\n",
    "\n",
    "# initializing tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# texts to tokens\n",
    "text = 'It is very handy to use transformers'\n",
    "ids = tokenizer.encode(text.lower(), add_special_tokens=True)\n",
    "ids = data['review'].apply(\n",
    " lambda x: tokenizer.encode(x.lower(), \n",
    "                             add_special_tokens=True, truncation=True, \n",
    "                             max_length=512))\n",
    "\n",
    "# padding (appending zero's to the vector to make its length equal to n)\n",
    "n = 512\n",
    "padded_ids = []\n",
    "for each in ids:\n",
    "    padded = np.array(each[:n] + [0]*(n - len(each)))\n",
    "    padded_ids.append(padded)\n",
    "\n",
    "# creating the attention mask to distinguish tokens we are interested in\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded_ids != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2472, 5203, 2909, 2243, 2320, 2056, 1036, 2045, 1005, 1055, 1037, 2200, 2460, 3292, 2090, 2152, 2396, 1998, 11669, 1010, 1998, 11669, 2008, 3397, 13675, 16103, 2791, 2003, 2011, 2023, 2200, 3737, 20388, 2000, 2396, 1005, 1012, 2023, 4861, 11859, 2010, 5988, 6669, 1010, 1037, 2200, 4310, 2303, 1997, 2147, 2008, 2950, 4438, 2754, 17241, 1010, 6172, 1998, 2162, 3152, 1010, 2530, 2015, 1998, 1997, 2607, 1010, 2010, 3297, 11463, 7716, 14672, 2015, 1012, 2909, 2243, 1005, 1055, 11463, 7716, 14672, 2015, 2020, 1010, 2004, 1996, 2200, 2773, 27353, 1010, 16547, 2007, 2189, 1012, 1996, 2189, 4520, 1996, 4309, 2005, 2010, 3040, 3993, 2806, 1010, 1998, 2296, 6909, 1997, 2010, 8248, 1006, 2909, 2243, 2001, 2036, 1037, 5276, 1007, 3727, 1037, 3928, 3746, 2006, 1996, 3898, 1011, 2357, 1011, 10683, 1012, 2021, 2023, 7110, 1005, 1056, 2166, 2021, 2049, 6630, 1010, 2019, 20017, 1997, 2166, 1012, 2909, 2243, 2196, 2699, 2000, 2265, 4507, 1010, 2006, 1996, 10043, 1012, 3904, 1997, 1996, 5501, 1997, 2010, 4245, 2081, 1037, 2488, 2224, 1997, 2035, 1996, 4087, 5733, 3024, 2011, 5365, 1006, 2087, 5546, 6627, 8713, 12898, 2099, 1007, 2000, 10782, 1996, 7976, 2013, 1996, 2613, 2518, 1012, 2292, 1005, 1055, 3342, 2008, 2010, 3585, 2558, 19680, 2015, 2007, 1996, 2051, 2043, 5365, 3152, 2357, 2049, 3086, 2046, 1996, 2591, 3689, 1006, 2304, 6277, 8894, 1010, 8443, 2302, 1037, 3426, 1007, 1012, 2909, 2243, 2467, 2354, 2008, 5988, 2001, 3214, 2000, 2022, 2242, 2842, 1012, 2178, 1997, 2909, 2243, 1005, 1055, 8635, 7680, 7849, 10057, 2023, 1024, 1036, 2017, 2064, 1005, 1056, 3362, 1010, 2030, 3543, 1010, 1996, 2613, 1012, 2017, 2074, 2156, 16055, 1012, 2065, 2017, 3046, 2000, 10616, 8404, 2993, 2115, 3093, 2069, 3113, 3221, 1005, 1012, 1045, 13366, 2100, 10334, 2008, 2038, 2464, 2517, 2006, 1996, 3612, 2000, 4175, 1996, 3815, 1997, 13536, 1998, 4871, 7686, 2008, 3711, 2006, 3898, 1012, 2028, 4515, 2039, 3228, 2039, 1012, 3568, 1010, 2057, 2024, 1999, 1037, 2534, 2440, 1997, 13536, 2073, 2045, 1005, 1055, 2053, 4489, 2090, 2613, 1998, 2049, 6270, 6100, 1012, 6343, 2064, 2360, 2008, 1996, 21681, 2024, 2613, 2111, 1012, 2008, 2237, 7110, 1005, 1056, 2613, 2593, 1010, 2007, 2216, 22293, 3514, 15856, 2035, 2058, 1996, 2173, 1012, 2061, 1999, 2023, 8391, 1996, 3772, 2003, 5360, 1010, 1996, 25545, 2063, 2003, 8275, 1010, 1996, 7577, 2003, 5710, 1012, 2673, 2003, 3724, 1037, 2210, 2978, 2125, 1996, 5787, 1006, 1996, 4424, 9530, 17048, 10708, 1997, 9984, 16321, 2007, 1996, 3514, 3578, 1010, 2005, 2742, 1007, 1012, 2909, 2243, 2001, 21289, 1998, 14833, 21885, 2075, 2012, 1996, 2168, 2051, 1012, 1036, 1996, 12113, 2024, 1996, 2472, 1005, 1055, 4301, 1025, 1996, 7497, 2003, 2010, 4695, 1005, 1012, 1999, 2517, 2006, 1996, 3612, 2057, 3582, 1996, 2991, 1997, 1037, 3151, 2126, 1997, 2166, 2119, 1999, 1037, 14965, 2389, 2126, 1998, 1999, 3408, 1997, 2422, 1998, 6281, 1012, 1996, 21681, 2015, 2160, 1010, 2007, 2049, 2367, 3798, 4198, 2011, 1996, 12313, 10714, 5748, 1999, 1037, 9975, 19240, 7476, 2126, 1012, 1037, 2160, 2008, 12950, 1037, 19049, 1010, 2008, 2053, 2283, 2064, 15138, 2039, 1012, 2004, 102]\n"
     ]
    }
   ],
   "source": [
    "print(ids[2026])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data['review_lemm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       i see this movie last year in medium class and...\n",
       "1       i must admit there be few book with correspond...\n",
       "2       i think that the shot and light be very poor w...\n",
       "3       a few week ago i read the classic george orwel...\n",
       "4       i see this movie literally directly after fini...\n",
       "                              ...                        \n",
       "2022    director douglas sirk score again with this th...\n",
       "2023    spoiler spoiler release in and consider quite ...\n",
       "2024    fabulous film rent the dvd recently and be flo...\n",
       "2025    rich alcoholic robert stack fall in love with ...\n",
       "2026    director douglas sirk once say ` there be a ve...\n",
       "Name: review_lemm, Length: 2027, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of vector IDs (padded) and the list of attention masks\n",
    "ids_list = []\n",
    "attention_mask_list = []\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "for input_text in corpus[:200]:\n",
    "    ids = tokenizer.encode(input_text.lower(), add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "    padded = np.array(ids + [0]*(max_length - len(ids)))\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    ids_list.append(padded)\n",
    "    attention_mask_list.append(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff8efd8977c4864b6d7b925d858fc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb7ca9144c641329e146251a3799db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "config = transformers.BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcead248f0184cddaa3961addadc9636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#tqdm lilbrary displays progress of  operation\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for i in tqdm(range(int(8e6))):\n",
    "    pass\n",
    "\n",
    "# the progress bar will appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The BERT model creates embeddings in batches\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a loop for the batches\n",
    "# creating an empty list of review embeddings \n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(len(ids_list) // batch_size)):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the data into a tensor format. \n",
    "# putting together vectors of ids (of tokens) to a tensor\n",
    "ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)])\n",
    "# putting together vectors of attention masks to a tensor\n",
    "attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_size*(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass the data and the mask to the model to obtain embeddings for the batch:\n",
    "batch_embeddings = model(ids_batch, attention_mask=attention_mask_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the no_grad() (no gradient) function to indicate that we don't need gradients in the torch library \n",
    "#it will make calculations faster\n",
    "with torch.no_grad():\n",
    "    batch_embeddings = model(ids_batch, attention_mask=attention_mask_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the required elements from the tensor and add the list of all the embeddings:\n",
    "# converting elements of tensor to numpy.array with the numpy() function\n",
    "embeddings.append(batch_embeddings[0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting all the above together, we get this loop:\n",
    "batch_size = 100\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(len(ids_list) // batch_size)):\n",
    "    \n",
    "    ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)])\n",
    "    attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_size*(i+1)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(ids_batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "    embeddings.append(batch_embeddings[0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all the embeddings in a matrix of features:\n",
    "features = np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The features are ready. Time to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-7af7222331ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpadded_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "padded_ids[0][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
