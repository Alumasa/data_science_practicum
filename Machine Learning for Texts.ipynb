{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer  = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heaven'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('heavens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"All models are wrong, but some are useful.\"\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'model', 'are', 'wrong', ',', 'but', 'some', 'are', 'useful', '.']\n"
     ]
    }
   ],
   "source": [
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all model are wrong , but some are useful .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-67741b070562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parser'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ner'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "doc = nlp(text.lower())\n",
    "\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization using spaCy\n",
    "import pandas as pd\n",
    "import random             # in order to select a random review\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small.tsv', sep='\\t')\n",
    "corpus = data['review']\n",
    "\n",
    "def lemmatize(text):\n",
    "\n",
    "    # < write code here >\n",
    "    text = nlp(text.lower())\n",
    "    lemmas = [token.lemma_ for token in text]\n",
    "    text_lemmas = \" \".join(lemmas)\n",
    "    return text_lemmas\n",
    "\n",
    "# store the review index in the review_idx variable\n",
    "# either as a random number or a fixed value, e.g. 2557 \n",
    "#review_idx = random.randint(0, len(corpus)-1)\n",
    "review_idx = 2557\n",
    "\n",
    "review = corpus[review_idx]\n",
    "\n",
    "print(\"The original text:\", review)\n",
    "print()\n",
    "print(\"The lemmatized text:\", lemmatize(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pattern' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-978b5ccfa074>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# substitution — what each pattern match should be substituted with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# text — the text which the function scans for pattern matches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pattern' is not defined"
     ]
    }
   ],
   "source": [
    "# pattern\n",
    "# substitution — what each pattern match should be substituted with\n",
    "# text — the text which the function scans for pattern matches\n",
    "re.sub(pattern, substitution, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "\n",
      "Hello!\\n\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello!\\n\")\n",
    "\n",
    "print(r\"Hello!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a range of letters is indicated by a hyphen:\n",
    "# a-z = abcdefghijklmnopqrstuvwxyz\n",
    "r\"[a-zA-Z]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find apostrophes as well\n",
    "r\"[a-zA-Z']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I liked this show from the first episode I saw  which was the  Rhapsody in Blue  episode  for those that don't know what that is  the Zan going insane and becoming pau lvl    ep   Best visuals and special effects I've seen on a television series  nothing like it anywhere  \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review text\n",
    "text = \"\"\"\n",
    "I liked this show from the first episode I saw, which was the \"Rhapsody in Blue\" episode (for those that don't know what that is, the Zan going insane and becoming pau lvl 10 ep). Best visuals and special effects I've seen on a television series, nothing like it anywhere.\n",
    "\"\"\"\n",
    "re.sub(r\"[^a-zA-Z']\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'liked', 'this', 'show']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"            I   liked   this   show   \"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I liked this show'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(['I', 'liked', 'this', 'show'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random             # in order to select a random review\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small.tsv', sep='\\t')\n",
    "corpus = data['review']\n",
    "\n",
    "def clear_text(text):\n",
    "    \n",
    "    # < write code here >\n",
    "    text = re.sub(r\"[^a-zA-Z']\", \" \", text)\n",
    "    text = text.split()\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        lemmas.append(token.lemma_)\n",
    "        \n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# store the review index in the review_idx variable\n",
    "# either as a random number or a fixed value, e.g. 2557 \n",
    "review_idx = random.randint(0, len(corpus)-1)\n",
    "# review_idx = 2557\n",
    "\n",
    "review = corpus[review_idx]\n",
    "\n",
    "print(\"The original text:\", review)\n",
    "print()\n",
    "print(\"The lemmatized text:\", lemmatize(clear_text(review)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "text = \"\"\"For want of a nail the shoe was lost. For want of a shoe the horse was lost. For want of a horse the rider was lost.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.lemma_ for token in doc if not token.is_punct]\n",
    "\n",
    "bow = Counter(tokens)\n",
    "\n",
    "vector = [bow[token] for token in sorted(bow)]\n",
    "\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "    Natural-language processing (NLP) is an area of\n",
    "    computer science and artificial intelligence\n",
    "    concerned with the interactions between computers\n",
    "    and human (natural) languages.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n natural language processing nlp',\n",
       " 'natural language processing nlp is',\n",
       " 'language processing nlp is an',\n",
       " 'processing nlp is an area',\n",
       " 'nlp is an area of\\n',\n",
       " 'is an area of\\n computer',\n",
       " 'an area of\\n computer science',\n",
       " 'area of\\n computer science and',\n",
       " 'of\\n computer science and artificial',\n",
       " 'computer science and artificial intelligence\\n',\n",
       " 'science and artificial intelligence\\n concerned',\n",
       " 'and artificial intelligence\\n concerned with',\n",
       " 'artificial intelligence\\n concerned with the',\n",
       " 'intelligence\\n concerned with the interactions',\n",
       " 'concerned with the interactions between',\n",
       " 'with the interactions between computers\\n',\n",
       " 'the interactions between computers\\n and',\n",
       " 'interactions between computers\\n and human',\n",
       " 'between computers\\n and human natural',\n",
       " 'computers\\n and human natural languages',\n",
       " 'and human natural languages \\n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ngrams(s, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', 'five']\n",
      "[['one', 'two', 'three', 'four', 'five'], ['two', 'three', 'four', 'five'], ['three', 'four', 'five']]\n",
      "[('one', 'two', 'three'), ('two', 'three', 'four'), ('three', 'four', 'five')]\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "s = \"one two three four five\"\n",
    "\n",
    "tokens = s.split(\" \")\n",
    "# tokens = [\"one\", \"two\", \"three\", \"four\", \"five\"]\n",
    "print(tokens)\n",
    "\n",
    "sequences = [tokens[i:] for i in range(3)]\n",
    "# The above will generate sequences of tokens starting\n",
    "# from different elements of the list of tokens.\n",
    "# The parameter in the range() function controls\n",
    "# how many sequences to generate.\n",
    "#\n",
    "# sequences = [\n",
    "#   ['one', 'two', 'three', 'four', 'five'],\n",
    "#   ['two', 'three', 'four', 'five'],\n",
    "#   ['three', 'four', 'five']]\n",
    "print(sequences)\n",
    "\n",
    "bigrams = zip(*sequences)\n",
    "print(list(bigrams))\n",
    "# The zip function takes the sequences as a list of inputs\n",
    "# (using the * operator, this is equivalent to\n",
    "# zip(sequences[0], sequences[1], sequences[2]).\n",
    "# Each tuple it returns will contain one element from\n",
    "# each of the sequences.\n",
    "# \n",
    "# To inspect the content of bigrams, try:\n",
    "# print(list(bigrams))\n",
    "# which will give the following:\n",
    "#\n",
    "# [\n",
    "#   ('one', 'two', 'three'),\n",
    "#   ('two', 'three', 'four'),\n",
    "#   ('three', 'four', 'five')\n",
    "# ]\n",
    "#\n",
    "# Note: even though the first sequence has 5 elements,\n",
    "# zip will stop after returning 3 tuples, because the\n",
    "# last sequence only has 3 elements. In other words,\n",
    "# the zip function automatically handles the ending of\n",
    "# the n-gram generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "s = s.lower()\n",
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "output = list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 'two', 'three'), ('two', 'three', 'four'), ('three', 'four', 'five')]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'for want of a nail the shoe be lose',\n",
    "    'for want of a shoe the horse be lose',\n",
    "    'for want of a horse the rider be lose',\n",
    "    'for want of a rider the message be lose',\n",
    "    'for want of a message the battle be lose',\n",
    "    'for want of a battle the kingdom be lose',\n",
    "    'and all for the want of a horseshoe nail'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The counter extracts unique words from the corpus and counts how many times they appear in each text of the corpus. \n",
    "#The counter doesn't count separate letters.\n",
    "# bow = bag of words\n",
    "bow = count_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 16)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape\n",
    "#16 unique words, 7 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1]\n",
      " [0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1]\n",
      " [0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1]\n",
      " [0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1]\n",
      " [1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'and',\n",
       " 'battle',\n",
       " 'be',\n",
       " 'for',\n",
       " 'horse',\n",
       " 'horseshoe',\n",
       " 'kingdom',\n",
       " 'lose',\n",
       " 'message',\n",
       " 'nail',\n",
       " 'of',\n",
       " 'rider',\n",
       " 'shoe',\n",
       " 'the',\n",
       " 'want']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The list of unique words in the bag is called a vocabulary. \n",
    "#It's stored in the counter and can be accessed by calling the get_feature_names() method:\n",
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams\n",
    "count_vect = CountVectorizer(ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2 = count_vect.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 1 0 1 1]\n",
      " [0 1 0 0 1 0 0 0 1 1]\n",
      " [0 1 0 0 1 0 0 1 0 1]\n",
      " [0 0 0 0 1 1 0 1 0 1]\n",
      " [1 0 0 0 1 1 0 0 0 1]\n",
      " [1 0 0 1 1 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['battle',\n",
       " 'horse',\n",
       " 'horseshoe',\n",
       " 'kingdom',\n",
       " 'lose',\n",
       " 'message',\n",
       " 'nail',\n",
       " 'rider',\n",
       " 'shoe',\n",
       " 'want']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# < write code here >\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small_lemm.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "# create a bag-of-words without checking for stop words\n",
    "# < write code here >\n",
    "count_vect = CountVectorizer()\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "print(\"The BoW size with stop words:\", bow.shape)\n",
    "\n",
    "# create a bag-of-words with checking for stop words\n",
    "# < write code here >\n",
    "stop_words = set(stopwords.words('english'))\n",
    "count_vect = CountVectorizer(stop_words=stop_words)\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(\"The BoW size without stop words:\", bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small_lemm.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "# create an n-gram with n=2 and store it in the n_gram variable\n",
    "\n",
    "# < write code here >\n",
    "count_vect = CountVectorizer(ngram_range=(2, 2))\n",
    "n_gram = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(\"The size of 2-gram:\", n_gram.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = count_tf_idf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "# import TfidfVectorizer\n",
    "# < write code here >\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = pd.read_csv('/datasets/imdb_reviews_small_lemm.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "stop_words = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "tf_idf = count_tf_idf.fit_transform(corpus)\n",
    "# < write code here >\n",
    "\n",
    "print(\"The TF-IDF matrix size:\", tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
